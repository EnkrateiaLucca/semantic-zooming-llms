{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. Paris is one of the most populous cities in Europe, and it is renowned worldwide for its art, fashion, gastronomy, and culture. It is the most popular tourist destination in the world, with more than 40 million tourists annually. Paris is also known for its beautiful cityscape, which includes architectural icons like the Eiffel Tower, the Louvre Museum, the Notre-Dame Cathedral, and the Palace of Versailles.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "def get_response(prompt,model_name=\"mixtral-8x7b-32768\", SYSTEM_PROMPT=\"You are a helpful assistant.\"):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),)\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "             },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "get_response(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = PyPDFLoader(\"./paper.pdf\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nYu Du1 *Fangyun Wei2 *†Hongyang Zhang3\\n1Tsinghua University2Microsoft Research Asia3University of Waterloo\\nduyu20@mails.tsinghua.edu.cn fawe@microsoft.com hongyang.zhang@uwaterloo.ca\\n*Equal contribution†Corresponding author\\nAbstract\\nWe introduce AnyTool, a large language model\\nagent designed to revolutionize the utilization of\\na vast array of tools in addressing user queries.\\nWe utilize over 16,000 APIs from Rapid API,\\noperating under the assumption that a subset of\\nthese APIs could potentially resolve the queries.\\nAnyTool primarily incorporates three elements:\\nan API retriever with a hierarchical structure, a\\nsolver aimed at resolving user queries using a se-\\nlected set of API candidates, and a self-reflection\\nmechanism, which re-activates AnyTool if the ini-\\ntial solution proves impracticable. AnyTool is\\npowered by the function calling feature of GPT-4,\\neliminating the need for training external modules.\\nWe also revisit the evaluation protocol introduced\\nby previous works and identify a limitation in this\\nprotocol that leads to an artificially high pass rate.\\nBy revising the evaluation protocol to better re-\\nflect practical application scenarios, we introduce\\nan additional benchmark, termed AnyToolBench.\\nExperiments across various datasets demonstrate\\nthe superiority of our AnyTool over strong base-\\nlines such as ToolLLM and a GPT-4 variant tai-\\nlored for tool utilization. For instance, AnyTool\\noutperforms ToolLLM by +35.4% in terms of\\naverage pass rate on ToolBench. Code will be\\navailable at https://github.com/dyabel/AnyTool.\\n1. Introduction\\nFrom the dawn of civilization, humanity has embarked on\\na relentless journey of discovery and innovation, mastering\\nan ever-expanding array of tools to enhance our capabilities\\nand increase production efficiency. As we have evolved,\\nso have our tools, transitioning from simple stone imple-\\nments to complex machines and beyond. Today, we stand\\nat the forefront of a new era, reaping the benefits of the\\nrapid developments in artificial intelligence, particularly the\\nrecent advances in large language models (LLMs) (Brown\\net al., 2020; Touvron et al., 2023a;b; Chowdhery et al., 2023;\\nAchiam et al., 2023; Ouyang et al., 2022). A pivotal chal-\\nAPI Pool ( 16K+ APIs)\\n…AnyTool\\nSelf-Reflection SolutionQuery\\nAPI-Retriever\\nSolver\\n(a) AnyTool addresses user queries by leveraging 16k+ APIs.\\nIt integrates a hierarchical API-retriever, a solver, and a self-\\nreflection mechanism in a closed loop, all operating without\\nthe need for additional training.\\n(b) Comparison with ToolLLM and a GPT-4 variant tailored for\\ntool utilization across six subsets of ToolBench (Qin et al., 2023b),\\nusing pass rate defined in Eq 2 as the evaluation metric.\\nFigure 1: (a) Illustration of AnyTool. (b) Comparison in\\nperformance.\\nlenge now is learning how to drive LLMs to effectively use\\ntools (Qin et al., 2023a; Xu et al., 2023; Cai et al., 2023;\\nSong et al., 2023; Ruan et al., 2023; Shen et al., 2023; Hao\\net al., 2023), a task that could redefine our interaction with\\ntechnology. Towards this end, we introduce AnyTool, a\\nGPT-4-empowered agent, as depicted in Figure 1a. It is\\ndesigned to effectively leverage more than 16,000 APIs to\\n1arXiv:2402.04253v1  [cs.CL]  6 Feb 2024', metadata={'source': './paper.pdf', 'page': 0}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nMeta-AgentCategory\\nAPI-Candidate P oolFunction L ist\\ncreate _agent_category_level (category_name )\\n…Query\\nget_tools_in_category (category_name )\\nget_tool_descriptions ([tool_list ])\\nfinish_search ( )\\nget_tools _in_category (category_name )\\nget_tool_descriptions ([tool_list ])\\nSports Music\\n Finance…\\nTool-1Tool-2Tool-3 Tool-M…\\nAPI-1API-2API-3 API-D\\nadd_API_into_API_pool ([API_name_list ])\\nget_API_details (API_name )\\ncheck_if_request_solvable ( )Tool API\\n API-2\\n API-1\\n API-D\\n ; …\\nStructure of 16K+ APIs:Category Agent\\n:Tool Agent\\nSelf-Reflection Solved\\n Solution\\n Solver Unsolved\\n…finish_search ( )finish_search ( )\\n…\\norAPI R etriever\\ncreate _agent_tool_level  ([tool_list ])\\nget_APIs _in_tool(tool_name )\\nFigure 2: Overview of AnyTool. It primarily consists of a hierarchical API retriever tasked with identifying the most\\nrelevant API candidates to the user query from a large API pool, a solver aimed at addressing the queries using the generated\\nAPI-candidate pool, and a self-reflection mechanism. The hierarchical structure includes a meta-agent linked with several\\ncategory agents, each of which manages a collection of tool agents. We leverage the API structure defined by Rapid API as\\na guideline. Each type of agent is assigned several functions that it can use to explore the API space. Refer to Table 8 in the\\nappendix for the details of each function.\\naddress user queries, with a significant performance leap as\\ndepicted in Figure 1b.\\nPrevious research (Qin et al., 2023b) formulated tool uti-\\nlization in a dual-phase approach: initially retrieving, then\\nresolving. Specifically, the first phase involves retrieving the\\nmost pertinent APIs from a substantial collection of 16K+\\nAPIs in response to user queries. The subsequent phase fo-\\ncuses on utilizing these chosen APIs to address user queries.\\nOur AnyTool uses this design principle while introducing\\nfour distinct characteristics (see Figure 2 for an overview):\\nPlug-and-Play. Our AnyTool does not require the training\\nof any modules, except for the function-calling feature of\\nGPT-4 (Achiam et al., 2023). This aspect sets it apart from\\nexisting methods like ToolLLM, which necessitates training\\nan API retriever capable of selecting a set of candidate APIs\\nfrom the API pool (Qin et al., 2023b).\\nHierarchical Structure. To identify the most relevant APIs\\nfor user queries, we design a hierarchical structure within\\nour API retriever. This structure is composed of three tiers,\\neach containing one or multiple agents with diverse roles.\\nThis arrangement is inspired by the divide-and-conquer ap-\\nproach. Additionally, we effectively incorporate the API\\ncategorization suggested by Rapid API into our hierarchical\\nstructure. Consequently, this significantly reduces the search\\nscope for each agent and overcomes constraints related to\\nthe maximum context length in LLMs.\\nFigure 3: The performance of our AnyTool on different\\ndatasets (each denoted by a curve) improves as the number\\nof self-reflection rounds increases. ATB: AnyToolBench.\\nSelf-Reflection Mechanism. Our AnyTool is designed to\\naddress user queries through a process of initial attempt\\nfollowed by reflection. Upon receiving a query, AnyTool\\nsuggests a solution, which is then evaluated for feasibility\\nby GPT-4. In cases where the proposed solution is deemed\\nimpractical, AnyTool is re-activated, with the considera-\\ntion of reasons for failure and relevant historical contexts.\\nThis mechanism significantly reduces the tendency to “over-\\nsearch” for simpler queries, while also providing a more\\ncontext-rich and in-depth search for complex queries. This\\nclosed-loop system enhances the efficiency and effective-\\nness of the query resolution process. Figure 3 shows how\\nthe pass rate improves w.r.t. the self-reflection rounds. With\\n2', metadata={'source': './paper.pdf', 'page': 1}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nonly 4-6 self-reflection iterations, the pass rate improves by\\nup to 20% across all datasets.\\nEvaluation for Realistic Scenarios. The evaluation frame-\\nwork presented in ToolBench (Qin et al., 2023b) commences\\nwith categorizing user queries as either solvable or non-\\nsolvable, employing a set of reference APIs. Following this,\\nthe solvable queries undergo further scrutiny to determine if\\nthey are successfully addressed or not. However, for those\\nnon-solvable queries, the evaluation system regards them as\\nsolved when calculating the pass rate, leading to an artifi-\\ncially high pass rate. Our study delves into the intricacies of\\nthis evaluation methodology and proposes a revised protocol\\nthat better mirrors practical application scenarios.\\nIn addition to evaluation on ToolBench, we introduce an\\nextra benchmark, termed AnyToolBench, to facilitate the\\napplication of our new evaluation protocol. Experimen-\\ntally, AnyTool achieves state-of-the-art performance, sur-\\npassing strong baselines such as ToolLLM and a version of\\nGPT-4 specifically tailored for tool utilization across various\\ndatasets, as illustrated in Figure 1b.\\n2. Related Works\\nTool Utilization in LLMs. Large language models (Rad-\\nford et al., 2018; 2019; Brown et al., 2020; Touvron et al.,\\n2023a;b; Thoppilan et al., 2022) may commit factual errors\\nwhen responding to queries, particularly struggling with pre-\\ncise numbers and specific fields of expertise (Huang et al.,\\n2023; Augenstein et al., 2023). Utilizing tools can help miti-\\ngate this issue (Li et al., 2023; Qin et al., 2023b; Parisi et al.,\\n2022; Tang et al., 2023; Hsieh et al., 2023; Schick et al.,\\n2023). Previous work has involved using an API retriever\\nto match relevant APIs from a large API pool based on the\\ndocuments, employing either an pretrained text embedding\\nmodel (Li et al., 2023; Patil et al., 2023) or one finetuned\\nwith curated API retrieval data (Qin et al., 2023b). How-\\never, this approach typically suffers from low accuracy and\\nmay overlook the truly relevant APIs. Moreover, there is a\\nlack of feedback mechanism in their retrieval, often leading\\nto unsolved queries due to incorrect API candidates being\\nprovided. Our AnyTool fills this gap by directly using the\\nGPT-4 as the API retriever with a hierarchical structure de-\\nsign, and introduces the self-reflection mechanism into the\\nwhole process.\\nSelf-Reflection Mechanism in LLMs. Self-reflection is a\\nfeatured ability of LLMs. It was first studied in the LLM\\nalignment problems. Wang et al. (2022) considered the\\nability of GPT-3 to self-generate instructions for alignment\\nfinetuning. Without finetuning, Li et al. (2024) introduced\\nan inference method, RAIN, that allows pre-trained LLMs to\\nevaluate their own generation and use the evaluation results\\nto guide rewind and generation for AI safety. Recently,\\nChen et al. (2024) proposed a self-play mechanism, where\\nthe LLM refines its capability by playing against instances ofitself. Yuan et al. (2024) proposed self-rewarding language\\nmodels, where the language model itself is used via LLM-\\nas-a-Judge prompting to provide its own rewards for the\\nfollowing DPO finetuning (Rafailov et al., 2023). On the\\nother hand, some negative results on self-reflection were\\nalso investigated. For example, Huang et al. (2023) showed\\nthat GPT-3.5-Turbo and GPT-4 cannot self-correct reasoning\\nyet. But whether GPT-4 can serve as a self-reflective agent\\nfor API calling remains an open problem in the existing\\nliterature.\\n3. Preliminaries\\n3.1. Function Calling\\nFunction calling is a core characteristic of GPT-4 (Achiam\\net al., 2023). Specifically, in response to a user’s query Q,\\nthe function calling system accesses a set of Mdistinct\\nfunctions {Fi}M\\ni=1. Each function Fihas the potential to\\nsolveQ, a part of Q, or may not be relevant to Qat all.\\nThe functionality of Fiis elaborated in a specific document\\nthat outlines its purpose, required and optional parameters\\nalong with their explanations, the types of output it gener-\\nates, and the interpretations of these outputs. Note that the\\nfunction calling feature of GPT-4 does not require visibility\\ninto the detailed implementations of each function. It under-\\nstands their intentions and functionalities through linguistic\\ncomprehension.\\nThe process of function calling involves: 1) the user inputs\\nboth the query Qand the function list {Fi}M\\ni=1, alongside\\na designated “Finish Function” F∗, into GPT-4; 2) GPT-\\n4 generates a function calling request for the user, with\\nclear input parameters; 3) the user executes the specific\\nfunction and provides the historical context and function\\nresponse to GPT-4; 4) this cycle of steps two and three is\\nrepeated multiple times until GPT-4 activates the “Finish\\nFunction” F∗, signaling the resolution of query Q. Users\\nhave the option to either employ the output of F∗directly,\\nor to gather the interim results generated during the function\\ncalling process, according to their specific goals or design.\\n3.2. Problem Formulation and Evaluation\\nProblem Formulation. The objective of this work is to de-\\nvelop a proficient agent capable of utilizing a vast collection\\nof real-world APIs to address user queries. We use over 16K\\nreal-world APIs from the RapidAPI Hub, as collected in the\\nToolLLM (Qin et al., 2023b). These APIs are represented as\\n{APIi}N\\ni=1, forming our API pool. The effectiveness of the\\nsolutions generated by the agent is assessed using GPT-4.\\nThis evaluation involves processing both the user query Q\\nand the proposed solution S, in accordance with established\\nevaluation protocols and criteria, to ascertain the solution’s\\nability to adequately address the query. We have also con-\\nducted human evaluation and find a correlation as high as\\n96.5% between GPT-4 and human evaluations.\\n3', metadata={'source': './paper.pdf', 'page': 2}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nQuery\\nAPI CandidatesAgent\\nQuerySolution\\nGPT-4+\\nSolvable\\nNon-solvable Solved Unsolved\\n…\\nAPI Pool ( 16K+ APIs)Agent\\nGPT-4Solution\\nSolved Unsolved\\nGPT-4\\nPass Rate =\\n+\\n+ +Pass Rate =\\n+\\n(a) Evaluation Protocol ( Prior Work ). (b) Evaluation Protocol ( Ours ).+\\nFigure 4: Illustration of the evaluation protocols used by: (a) ToolLLM (Qin et al., 2023b); and (b) ours. In (a), if the API\\nretriever selects candidates completely unrelated to the user’s query, GPT-4 may classify all queries as “non-solvable”,\\nleading to an artificially high pass rate, despite the queries remaining unsolved. In (b), we conduct a manual review of all\\nqueries and retain only those queries that can be resolved with specific APIs from the API pool for ToolBench.\\nEvaluation Protocol. We first revisit the evaluation pro-\\ntocol initially introduced by ToolLLM (Qin et al., 2023b).\\nToolLLM employs a dual-phase approach for utilizing vari-\\nous APIs. In the first phase, an API retriever is developed\\nto select the most relevant API candidates from the API\\npool according to a user query Q. The second phase in-\\nvolves ToolLLaMA, a specialized agent that formulates a\\nsolution using the selected API candidates. Due to its dual-\\nphase nature, ToolLLM’s evaluation is twofold. Initially,\\nGPT-4 evaluates whether the selected API candidates can\\naddress the query Q, categorizing them as either “solvable”\\nor “non-solvable”. If a query is deemed “solvable”, GPT-\\n4 then assesses the effectiveness of the provided solution,\\nclassifying it as either “solved” or “unsolved”. Figure 4(a)\\nillustrates how the pass rate Ris calculated:\\nR=#(Non-solvable ) +#(Solved )\\n#(Non-solvable ) +#(Solved ) +#(Unsolved ).(1)\\nHowever, a significant flaw exists in this evaluation protocol.\\nIf the API retriever selects candidates completely unrelated\\nto the user’s query, GPT-4 may classify all queries as “non-\\nsolvable”, leading to an artificially high pass rate, despite\\nthe queries remaining unsolved. Our experimental evidence\\nconfirms this issue, showing that when API candidates are\\nrandomly selected for each query, GPT-4 predominantly\\nlabels them as “non-solvable”, resulting in an inflated pass\\nrate of 99.0% through the metric defined in Eq 1.\\nTo address the limitations inherent in ToolLLM’s evaluation\\nprotocol, we propose an alternative evaluation methodol-\\nogy that aligns more closely with real-world scenarios, as\\nillustrated in Figure 4(b). Specifically, we bypass the first\\nevaluation phase of ToolLLM, which assesses the potential\\nof candidate APIs in addressing query Q. Instead, we di-\\nrectly utilize GPT-4 to determine the efficacy of the agent’s\\nproposed solution in resolving the query. The pass rate Risthus calculated using the formula:\\nR=#(Solved )\\n#(Solved ) +#(Unsolved ). (2)\\nTo ensure that all queries in the benchmark, namely Tool-\\nBench (Qin et al., 2023b), are solvable using certain APIs\\nfrom the API pool, we conduct a manual review of all\\nqueries. We retain only those queries that can be resolved\\nwith specific APIs from this pool. The detailed process is\\navailable in Section A.7 of the appendix.\\n4. AnyTool\\nOur AnyTool exhibits several distinctive features: Firstly, it\\neliminates the need for training external modules, and solely\\nrelies on the function calling feature of GPT-4. Secondly,\\nit can directly search the entire API pool, which contains\\nover 16K APIs, using a hierarchical structure and a divide-\\nand-conquer principle. Lastly, it is capable of self-reflection,\\nenabling it to review and analyze unsolved user queries by\\ntaking into account reasons for failure and relevant historical\\ncontexts.\\nOverview. The overview of AnyTool is depicted in Fig-\\nure 2. It primarily follows a three-step process to efficiently\\nresolve the user query Q. The first step (Section 4.1) in-\\nvolves the creation of an API candidate pool. For efficiency,\\nAnyTool is designed with a hierarchical architecture, taking\\nadvantage of the structured API organization available in\\nRapid API. In the second step (Section 4.2), a solver at-\\ntempts to resolve query Qby utilizing these API candidates.\\nFinally, if the query remains unsolved, AnyTool engages\\nin a self-reflection process (Section 4.3) in an attempt to\\nresolve it. A case study is shown in Section C.\\n4.1. API Retriever\\nStructured API Organization in Rapid API. Rapid API\\nemploys a structured system to categorize its extensive col-\\nlection of 16K+ APIs. Specifically, this organization is\\n4', metadata={'source': './paper.pdf', 'page': 3}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\ndivided into three distinct tiers: the first tier is the category\\nlevel, encompassing various domains such as “sports” and\\n“finance”; the second tier, designated as the tool level, con-\\nsists of tools that belong to specific categories; and the third\\ntier focuses on individual APIs, with each API belonging\\nto a specific tool, as illustrated in Figure 2. This hierarchi-\\ncal arrangement serves as a foundational guideline in the\\ndevelopment of our API retriever.\\nHierarchical Structure. As depicted in Figure 2, the struc-\\nture of our API retriever consists of three tiers. At the initial\\ntier, a meta-agent exists, tasked with dynamically generat-\\ning a series of category agents in response to the user query\\nQ. The intermediary tier is comprised of multiple category\\nagents, each established by the meta-agent. These agents\\ncorrespond to individual categories as defined by Rapid\\nAPI, with their primary objective being to identify the most\\nrelevant tools for the query Qfrom their respective tool\\ncollections. Subsequently, these category agents initiate the\\ncreation of various tool agents. It is important to note that\\neach tool agent may manage multiple tools, depending on\\nthe decisions made by the category agents. The goal of each\\ntool agent is to search through its managed APIs for those\\nthat might solve the query Q, and then add these APIs to an\\nAPI-candidate pool. Each type of agent possesses its own\\ndistinct set of functions. These are illustrated in Figure 2\\nand further detailed in Table 8 in the appendix.\\nGeneration of API-Candidate Pool. AnyTool is initiated\\nupon receiving a query Q, the function list detailed in Ta-\\nble 8, and a bootstrap prompt as outlined in Section B.1 of\\nthe appendix. This process heavily relies on the function\\ncalling feature of GPT-4 (refer to Section 3.1). Operating\\ninteractively, our system enables agents (starting with the\\nmeta-agent) to send requests for calling their managed func-\\ntions. These functions may involve creating a specific agent\\n(either a category agent or a tool agent) or executing a par-\\nticular function, in accordance with the historical context.1\\nThe requests are parsed, and the corresponding functions\\nare executed. The results produced by these functions are\\nsubsequently incorporated into the historical context, which\\nis then returned to the agents. This process repeats contin-\\nuously until the termination criteria are met. All agents,\\nincluding meta-agents, category agents, and tool agents,\\noperate independently in a multi-threaded manner, signifi-\\ncantly accelerating the process. We maintain a global API\\ncandidate pool, allowing each tool agent to add APIs to\\nthis pool, using the function “ addAPIinto APIpool ”\\n(refer to Figure 2 and Table 8). All agents cease\\noperations only when a tool agent calls the function\\n“check ifrequest solvable ” and receives a return\\nvalue of “True”. Subsequently, an API-candidate pool is ob-\\ntained. In addition, we record the historical context and sta-\\n1Each agent, whether it is a meta-agent, category agent, or tool\\nagent, maintains its own historical context independently.tus of each agent. An agent’s status is marked as “Finished”\\nonly if it calls the function “ finish search ” during the\\nprocess. Agents marked as “Finished” are excluded in the\\nself-reflection process, which will be described later.\\n4.2. Solver\\nFunctionality. The primary goal of the solver is to ad-\\ndress the user’s query Q, utilizing the generated API candi-\\ndate pool. It is implemented as a singular agent that lever-\\nages the function-calling capabilities inherent in GPT-4.\\nTwo potential implementations for the solver are the Depth-\\nFirst Search-Based Decision Tree (DFSDT) or the Chain\\nof Thought (CoT) approach. A concise overview of the\\nprocess is provided, with comprehensive details available\\nin ToolLLM (Qin et al., 2023b). The solver activates upon\\nreceiving a query Q, in conjunction with a suite of func-\\ntions, which includes those from the API candidate pool and\\na distinctive function named “ finish ”, as well as a boot-\\nstrap prompt detailed in Section B.2 of the appendix. The\\n“finish ” function yields one of three possible outcomes:\\n“Give Solution”, “Try Backtrack”, or “Give Up”, with “Try\\nBacktrack” being specific to the DFSDT implementation.\\nEach iteration involves: 1) the solver sending a request to\\ncall a function, 2) the interpretation of this request and the\\nexecution of the function, and 3) the integration of the func-\\ntion’s outcomes into the contextual history, which is then\\nreturned to the solver. This cycle continues until the solver\\ngives a “Give Solution” or “Give Up” decision. Note that\\nwhen the solver makes a “Give Up” decision, it is required\\nto provide both the reason and the function name of the APIs\\nthat are irrelevant to the user’s query or do not work properly.\\nSelf-reflection mechanism is triggered under two scenarios:\\n1) “Give Solution”, where GPT-4 reviews the solution and\\ndetermines that the query remains unresolved, and 2) “Give\\nUp”, where the solver fails to address the query.\\n4.3. Self-Reflection Mechanism\\nIf the initial solution fails to resolve user queries, the self-\\nreflection mechanism re-activates AnyTool sequentially, first\\nactivating the API retriever and then the solver. It is worth\\nnoting that this mechanism can be applied repeatedly until\\nthe termination condition is met.\\nSelf-Reflection in the API Retriever. Our self-reflection\\nmechanism first identifies the reason why a user query re-\\nmains unsolved. In instances where the solver opts to “Give\\nUp”, the rationale provided by the solver is utilized. Con-\\nversely, if the solver proposes a solution but GPT-4 assesses\\nthat it does not adequately address the query, the reasoning\\nascribed by GPT-4 is employed. Recall that we maintain a\\nrecord of historical context for each agent within the API re-\\ntriever. We initially incorporate the identified reason into all\\nthese historical contexts. Owing to the hierarchical design\\nof our API retriever, we systematically re-activate various\\nagents for efficiency purposes, following an ascending order\\n5', metadata={'source': './paper.pdf', 'page': 4}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 1: Main results on the filtered ToolBench. We use pass rate defined in Eq 2 and illustrated in Figure 4(b), as the metric.\\nAll results are reproduced. *: OpenAI’s text-embedding-ada-002; Ref.: reference; Avg.: average; SR: self-reflective.\\nModel API Retriever SolverUse Ref.\\nAPIsG1 G2 G3\\nAvg. (%)I (%) T (%) C (%) I (%) C (%) I (%)\\nToolLLM OpenAI TE∗ToolLLaMA w/ DFSDT 8.7 6.8 12.0 4.7 8.2 10.5 8.5\\nToolLLM ToolLLM’s ToolLLaMA w/ DFSDT 28.4 26.3 38.4 21.5 15.1 7.7 22.9\\nToolLLM ToolLLM’s GPT-4 w/ DFSDT 42.6 46.2 51.4 23.4 24.5 2.6 31.8\\nToolLLM None ToolLLaMA w/ DFSDT ✓ 29.4 31.8 37.1 19.6 22.4 13.2 25.6\\nGPT-4 None GPT-4 w/ CoT ✓ 31.3 34.8 47.1 27.1 34.7 2.6 29.6\\nGPT-4 None GPT-4 w/ DFSDT ✓ 36.5 49.2 51.4 38.3 39.8 18.4 38.9\\nGPT-4 Plain Agent GPT-4 w/ DFSDT 13.9 23.5 17.6 13.9 9.2 13.2 15.2\\nGPT-4 AutoGen-RAG GPT-4 w/ DFSDT 14.8 19.7 19.7 7.4 9.2 7.9 13.1\\nGPT-3.5 None GPT-3.5 w/ CoT ✓ 37.5 37.1 42.9 24.3 22.4 5.3 28.3\\nGPT-3.5 None GPT-3.5 w/ DFSDT ✓ 39.1 40.2 48.6 31.8 25.5 15.8 33.5\\nAnyTool (Ours) SR Agent SR GPT-4 w/ DFSDT 52.2 61.4 67.6 58.9 45.9 63.2 58.2\\nfrom tool agents, to category agents, and finally to the meta-\\nagent. It is worth noting that only the agents not marked\\nwith a “Finished” status are re-activated. As a result, this\\nprocess expands our API-candidate pool, incorporating new\\nAPIs that could potentially resolve the user’s query.\\nSelf-Reflection in the Solver. Recall that when the solver\\nmakes a “Give Up” decision, it is designed to identify the\\nfunction names of the APIs that are irrelevant to the user’s\\nquery. For efficiency, we first remove these APIs from the\\nexpanded API-candidate pool and exclude items where these\\nAPIs are called from the historical context of the solver. The\\nsolver is then re-activated with a new bootstrap prompt (refer\\nto Section B.3 in the appendix), the updated API-candidate\\npool, and the cleaned historical context. The remaining\\nprocess is the same as described in Section 4.2.\\n5. Experiments\\n5.1. Setup\\nBenchmarks. We conduct experiments on two benchmarks:\\n1)ToolBench (Qin et al., 2023b); and 2) our own benchmark,\\ntermed AnyToolBench . ToolBench comprises six subsets:\\nG1-Instruction (G1-I), G1-Tool (G1-T), G1-Category (G1-\\nC), G2-Instruction (G2-I), G2-Category (G2-C), and G3-\\nInstruction (G3-I). As described at the end of Section 3.2,\\nwe perform a manual review on ToolBench to exclude non-\\nsolvable queries. Details of this process can be found in\\nSection A.7 of the appendix. After filtering, the remaining\\nqueries in these six subsets are 115, 132, 142, 107, 98, and\\n38, respectively. Unless otherwise specific, we adopt the fil-\\ntered ToolBench . Our benchmark, AnyToolBench , includes\\n400 instances. The process of creating AnyToolBench is\\ndetailed in Section A.8 of the appendix.\\nEvaluation Protocol. We employ the pass rate (as defined\\nin Eq. 2) as our evaluation metric. To assess whether a\\nsolution generated by an agent can resolve the query, we\\nuse GPT-4-32K. The same prompt utilized in ToolBench is\\napplied when GPT-4 serves as the judge.Alignment between GPT-4’s Decisions and Decisions\\nMade by Human Evaluators. We conduct a compara-\\ntive analysis between decisions made by human evaluators\\nand those generated by GPT-4, focusing on samples from\\nthe G1-I subset of ToolBench. Specifically, for each query\\nsample, AnyTool generates a solution, which is then as-\\nsessed for its feasibility in addressing the query by both\\nhuman evaluators and GPT-4. Our results reveal that GPT-\\n4’s alignment with human evaluation stands at 96.5%, while\\nthat of GPT-3.5 is only 73.9%. Based on these findings, we\\nexclusively utilize GPT-4 for our evaluations.\\n5.2. Main Results\\nWe compare our AnyTool with the pioneering Tool-\\nLLM (Qin et al., 2023b) and its variants, as well as various\\nGPT-4 models tailored for tool utilization.\\nToolLLM and Its Variants. ToolLLM integrates an API\\nretriever2and a solver designed to address user queries by\\nemploying API candidates produced by the retriever. The\\nsolver operates using a finely-tuned LLaMA model, named\\nToolLLaMA, and employs a depth-first search-based deci-\\nsion tree (DFSDT) algorithm to resolve queries. For each\\nquery, ToolBench provides a set of reference APIs that are\\npotentially relevant. These reference APIs offer a means to\\nevaluate the solver’s effectiveness by allowing the bypassing\\nof the API retriever step. It is worth noting that additional\\nAPIs from the complete API pool, containing over 16,000\\nAPIs, may also contribute to effectively resolving queries.\\nBeyond the original ToolLLM, our experiments also ex-\\namine two variants: 1) one that substitutes ToolLLaMA\\nwith GPT-4 in the solver; 2) another that foregoes the API\\nretriever and relies solely on reference APIs.\\nVarious GPT-4 Models. The function-calling feature of\\nGPT-4 enables it to use APIs directly for resolving user\\nqueries. However, in our setting, we deal with over 16,000\\n2ToolLLM’s API retriever is trained on pair-wise data. Each\\npair includes a user query and a set of APIs relevant to the query.\\n6', metadata={'source': './paper.pdf', 'page': 5}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 2: Main results on our AnyToolBench. All models use\\nDFSDT implementation in the solver. SR: self-reflective;\\nPR: pass rate.\\nMethod API Retriever Solver PR (%)\\nToolLLM ToolLLM’s ToolLLaMA 18.9\\nToolLLM ToolLLM’s GPT-4 36.6\\nGPT-4 Plain Agent GPT-4 14.0\\nAnyTool (Ours) SR Agent SR GPT-4 73.8\\nAPIs. Integrating all these APIs—each with its unique\\nfunction description, input, and output—into GPT-4 si-\\nmultaneously exceeds the maximum context length of the\\nmodel, even for the version with the largest context length of\\n128,000 tokens. Therefore, we compare four GPT-4 models:\\n1) one that uses reference APIs and the Chain of Thought\\n(CoT) (Wei et al., 2022) algorithm in the solver; 2) another\\nthat uses reference APIs and the DFSDT algorithm; 3) a\\nthird that employs a plain agent for API retrieval and in-\\ncorporates the DFSDT algorithm in the solver; 4) a fourth\\nthat leverages the Retrieval Augmented Generation (RAG)\\nfeature from AutoGen (Augenstein et al., 2023) for API\\nretrieval, and uses the DFSDT algorithm to resolve user\\nqueries through the selected API candidates.\\nIn the implementation of GPT4-plain-agent, we divide the\\nset of over 16K APIs into 33 groups, each containing 500\\nAPIs, with the exception of the 33rd group. These groups are\\nthen sequentially processed by GPT-4. The specific task as-\\nsigned to GPT-4 involves identifying the relevant APIs using\\ntheaddAPIinto APIpool function, which integrates\\nthem into the API-candidate pool. Refer to Section A.4 for\\nmore details. Information on AutogGen-RAG can be found\\nin Section A.5.\\nMain Results on ToolBench. In Table 1, we compare our\\nAnyTool with various ToolLLM variants and GPT-4 models\\nacross six subsets of the filtered ToolBench dataset. The re-\\nsults on the original ToolBench are available in Section A.3.\\nBoth the API retriever and the solver contribute to the final\\nperformance. The API retriever’s role is to efficiently iden-\\ntify the most pertinent APIs from an extensive collection,\\nwhile the solver is tasked with generating viable solutions\\nfor user queries. Instead of training an API retriever as\\nToolLLM does, we leverage the powerful function-calling\\nfeature of GPT-4 and overcome the challenge posed by its\\ninherent maximum context length limitation, through the im-\\nplementation of a hierarchical structure. Our self-reflection\\nmechanism applies to both the API retriever and the solver,\\nenabling the whole system to operate in a closed loop. Ow-\\ning to these factors, our AnyTool significantly outperforms\\nboth the original ToolLLM and GPT-4 using reference APIs,\\nby +32.6 and +19.3 points, respectively, in terms of the\\naverage pass rate.\\nMain Results on AnyToolBench. AnyToolBench evaluatesTable 3: Ablation study on the pass rate of main components.\\n“-” and “+” symbols denote the removal and addition of a\\ncomponent from and into AnyTool, respectively.\\nConfiguration G2-I (%) G3-I (%)\\nAnyTool 58.9 63.2\\n-Hierarchical Structure 22.4 15.8\\n-Self-Reflection 19.6 15.8\\n-DFSDT/+CoT 50.5 60.3\\nTable 4: Ablation study on the pass rate of self-reflection\\nmechanism. All agents include the tool agents, the category\\nagents and the meta-agent.\\nRe-Activation G2-I (%) G3-I (%)\\nTool Agents 43.9 44.7\\nTool Agents + Category Agents 55.2 55.3\\nAll Agents 58.9 63.2\\nan agent’s capability to resolve user queries utilizing the\\nentire API pool. Consequently, an API retriever is essential\\nin this setting. We do not supply reference APIs for each\\nquery; thus, making comparisons with counterparts lacking\\nan API retriever is impractical. In Table 2, we compare\\nour AnyTool with a top-performing ToolLLM variant and\\nGPT-4, where a plain agent serves as the retriever. The\\nconsistent improvements demonstrated by AnyTool over\\nthese approaches affirm its effectiveness in a realistic setting.\\n5.3. Ablation Studies\\nUnless otherwise specific, all ablation studies are conducted\\non G2-I and G3-I of the filtered ToolBench.\\nEffectiveness of the Main Elements. Our AnyTool com-\\nprises two principal elements: firstly, an API retriever with a\\nhierarchical structure, and secondly, a self-reflection mecha-\\nnism. In Table 3, we examine three distinct configurations\\nof AnyTool. These include: a) substituting our hierarchical\\nAPI retriever with a flat-structure version, which merges the\\nfunctions of agents at the category and tool levels (except for\\n“agent creation” and “finish search” functions) into the func-\\ntion list of the meta-agent; b) eliminating the self-reflection\\nmechanism; and c) substituting the DFSDT algorithm with\\nCoT, thereby disabling the backtracking feature in DFSDT.\\nOur findings demonstrate significant positive effects of both\\nthe hierarchical structure and the self-reflection feature on\\nAnyTool’s performance. Choosing CoT over DFSDT results\\nin a decline in pass rates by 8.4 and 2.9, respectively.\\nSelf-Reflection Mechanism. In Section 4.3, we introduce\\na self-reflection mechanism that is first applied to the API\\nretriever module. It re-activates various agents in ascending\\norder, from tool agents to category agents, and finally to the\\nmeta-agent. In Table 4, we examine the different versions\\nthat reactivate distinct types of agents. Reactivating all\\nagents results in the best performance, owing to the larger\\nsearch space.\\nSize of the API Pool. Users typically submit a wide range\\n7', metadata={'source': './paper.pdf', 'page': 6}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 5: Study on the effects of the API pool’s size to the\\npass rate.\\nSize of API Pool G2-I (%) G3-I (%)\\n1,000 18.6 7.9\\n5,000 26.3 23.7\\n10,000 38.1 36.8\\nAll 58.9 63.2\\nTable 6: Study on the maximal size of API-candidate pool.\\nMaximal Size of API-Candidate Pool G2-I (%) G3-I (%)\\n16 49.5 42.1\\n32 58.9 55.3\\n64 58.9 63.2\\nTable 7: We study the maximum number of tools that a tool\\nagent can manage in our API retriever.\\nMaximum Number of Tools G2-I (%) G3-I (%)\\n3 48.6 42.1\\n5 58.9 57.9\\n10 52.3 39.5\\nof queries to the AI system, seeking solutions to real-world\\nproblems. To effectively address these queries, the sys-\\ntem requires access to a diverse array of APIs. In general,\\na larger API pool is more likely to successfully resolve\\nuser queries, as it offers a higher probability of containing\\nrelevant APIs. This hypothesis is evaluated by randomly\\nselecting subsets of APIs from the complete pool and using\\nonly these subsets to address user queries with our AnyTool.\\nThe results, presented in Table 5, support our hypothesis.\\nMaximal Size of the API-Candidate Pool. AnyTool op-\\nerates through a two-step process—the solver addresses\\nqueries by using an API-candidate pool, which is generated\\nby our hierarchical API Retriever. One termination criterion\\nfor the API retriever is the fullness of this pool. We examine\\nthe impact of the maximal size of the API-candidate pool as\\nshown in Table 6. We observe that a pool size of 64 nearly\\nreaches saturation in terms of performance.\\nTool Agent in API retriever. Our API retriever is designed\\nwith a hierarchical structure, in which the tool agents at the\\nbottom layer directly add APIs that may potentially address\\nuser queries, into the API-candidate pool. As described\\nin Section 4.1, a tool agent can manage a maximum of K\\ntools existing in Rapid API. We examine the value of Kin\\nTable 7. A trade-off is observed: managing too many tools\\n(e.g.,K= 10 ) leads to a larger search space and may cause\\noverlooking of relevant APIs, while managing too few tools\\n(e.g.,K= 3) might result in lower recall.\\nStatistics of Self-Reflection Frequency. In Figure 5, we\\nreport the average self-reflection frequency across all in-\\nstances within each subset of the filtered ToolBench and\\nour AnyToolBench. As described in Section 4.3, we re-\\nactivate various agents in ascending order. Consequently,\\nthe frequency of tool agents is much higher than that of\\nFigure 5: Statistics of average self-reflection frequency.\\nATB: AnyToolBench.\\nFigure 6: Statistics of average agent quantity.\\ncategory agents and meta-agent. Additionally, calculating\\nthe processing time for resolving queries with AnyTool is\\ninfeasible. AnyTool relies on the function-calling feature of\\nGPT-4, whose server response is often unstable.\\nAgent Quantity in API Retriever. The API retriever of\\nAnyTool is hierarchically structured. Depending on the na-\\nture of user queries, the meta-agent can dynamically create\\na varying number of category agents. This process is anal-\\nogous to the way category agents create tool agents. The\\naverage number of agents across all instances in each subset\\nof the filtered ToolBench and our AnyToolBench is depicted\\nin Figure 6.\\n6. Conclusion\\nIn this work, we introduce AnyTool, an advanced agent capa-\\nble of harnessing 16K+ APIs to effectively handle realistic\\nuser inquiries. The core of AnyTool is a hierarchical API re-\\ntriever coupled with a solver. Additionally, it incorporates a\\nunique self-reflection mechanism, enhancing its proficiency\\nin responding to user queries. We also revise the prior\\nevaluation protocol to better reflect real-world application\\nscenarios. Rigorous experiments conducted on ToolBench\\nand our AnyToolBench demonstrate our approach’s supe-\\nriority over established models. Finally, we highlight two\\nfuture research directions: 1) optimizing the organization\\nof APIs for improved performance and efficiency; 2) devel-\\n8', metadata={'source': './paper.pdf', 'page': 7}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\noping an advanced open-source LLM specifically for API\\nutilization, which could facilitate local deployments.\\nImpact Statements\\nAlthough AnyTool significantly enhances the effectiveness\\nof resolving user queries through various tools, its perfor-\\nmance in extremely complex scenarios has not been verified,\\nowing to the absence of appropriate datasets. Furthermore,\\nas AnyTool relies on the function-calling feature of GPT-4,\\nthe capabilities of GPT-4 also affect the feasibility of the\\nsolutions it generates.\\nReferences\\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\\nAnadkat, S., et al. GPT-4 technical report. arXiv preprint\\narXiv:2303.08774 , 2023.\\nAugenstein, I., Baldwin, T., Cha, M., Chakraborty, T.,\\nCiampaglia, G. L., Corney, D., DiResta, R., Ferrara,\\nE., Hale, S., Halevy, A., et al. Factuality challenges\\nin the era of large language models. arXiv preprint\\narXiv:2310.05189 , 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nCai, T., Wang, X., Ma, T., Chen, X., and Zhou, D.\\nLarge language models as tool makers. arXiv preprint\\narXiv:2305.17126 , 2023.\\nChen, Z., Deng, Y ., Yuan, H., Ji, K., and Gu, Q. Self-play\\nfine-tuning converts weak language models to strong lan-\\nguage models. arXiv preprint arXiv:2401.01335 , 2024.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. PaLM: Scaling language modeling\\nwith pathways. Journal of Machine Learning Research ,\\n24(240):1–113, 2023.\\nHao, S., Liu, T., Wang, Z., and Hu, Z. ToolkenGPT: Aug-\\nmenting frozen language models with massive tools via\\ntool embeddings. arXiv preprint arXiv:2305.11554 , 2023.\\nHsieh, C.-Y ., Chen, S.-A., Li, C.-L., Fujii, Y ., Ratner, A.,\\nLee, C.-Y ., Krishna, R., and Pfister, T. Tool documen-\\ntation enables zero-shot tool-usage with large language\\nmodels. arXiv preprint arXiv:2308.00675 , 2023.\\nHuang, J., Chen, X., Mishra, S., Zheng, H. S., Yu,\\nA. W., Song, X., and Zhou, D. Large language mod-els cannot self-correct reasoning yet. arXiv preprint\\narXiv:2310.01798 , 2023.\\nLi, M., Zhao, Y ., Yu, B., Song, F., Li, H., Yu, H., Li, Z.,\\nHuang, F., and Li, Y . API-Bank: A comprehensive bench-\\nmark for tool-augmented LLMs. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural Lan-\\nguage Processing , pp. 3102–3116, 2023.\\nLi, Y ., Wei, F., Zhao, J., Zhang, C., and Zhang, H. RAIN:\\nYour language models can align themselves without fine-\\ntuning. In International Conference on Learning Repre-\\nsentations , 2024.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nParisi, A., Zhao, Y ., and Fiedel, N. TALM: Tool augmented\\nlanguage models. arXiv preprint arXiv:2205.12255 ,\\n2022.\\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla:\\nLarge language model connected with massive apis. arXiv\\npreprint arXiv:2305.15334 , 2023.\\nQin, Y ., Hu, S., Lin, Y ., Chen, W., Ding, N., Cui, G., Zeng,\\nZ., Huang, Y ., Xiao, C., Han, C., et al. Tool learning with\\nfoundation models. arXiv preprint arXiv:2304.08354 ,\\n2023a.\\nQin, Y ., Liang, S., Ye, Y ., Zhu, K., Yan, L., Lu, Y ., Lin, Y .,\\nCong, X., Tang, X., Qian, B., et al. ToolLLM: Facilitating\\nlarge language models to master 16000+ real-world APIs.\\narXiv preprint arXiv:2307.16789 , 2023b.\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\\net al. Improving language understanding by generative\\npre-training. OpenAI , 2018.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI , 2019.\\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\\nC. D., and Finn, C. Direct preference optimization: Your\\nlanguage model is secretly a reward model. arXiv preprint\\narXiv:2305.18290 , 2023.\\nRuan, J., Chen, Y ., Zhang, B., Xu, Z., Bao, T., Du, G., Shi,\\nS., Mao, H., Zeng, X., and Zhao, R. TPTU: Task planning\\nand tool usage of large language model-based ai agents.\\narXiv preprint arXiv:2308.03427 , 2023.\\nSchick, T., Dwivedi-Yu, J., Dess `ı, R., Raileanu, R., Lomeli,\\nM., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-\\nformer: Language models can teach themselves to use\\ntools. arXiv preprint arXiv:2302.04761 , 2023.\\n9', metadata={'source': './paper.pdf', 'page': 8}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nShen, Y ., Song, K., Tan, X., Li, D., Lu, W., and Zhuang,\\nY . HuggingGPT: Solving ai tasks with ChatGPT and its\\nfriends in huggingface. arXiv preprint arXiv:2303.17580 ,\\n2023.\\nSong, Y ., Xiong, W., Zhu, D., Li, C., Wang, K., Tian, Y .,\\nand Li, S. RestGPT: Connecting large language models\\nwith real-world applications via RESTful APIs. arXiv\\npreprint arXiv.2306.06624 , 2023.\\nTang, Q., Deng, Z., Lin, H., Han, X., Liang, Q., and\\nSun, L. ToolAlpaca: Generalized tool learning for lan-\\nguage models with 3000 simulated cases. arXiv preprint\\narXiv:2306.05301 , 2023.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\\nDu, Y ., et al. LaMDA: Language models for dialog appli-\\ncations. arXiv preprint arXiv:2201.08239 , 2022.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation lan-\\nguage models. arXiv preprint arXiv:2302.13971 , 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288 ,\\n2023b.\\nWang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A.,\\nKhashabi, D., and Hajishirzi, H. Self-instruct: Aligning\\nlanguage model with self generated instructions. arXiv\\npreprint arXiv:2212.10560 , 2022.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\\nChi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems , 35:\\n24824–24837, 2022.\\nXu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang,\\nJ. On the tool manipulation capability of open-source\\nlarge language models. arXiv preprint arXiv:2305.16504 ,\\n2023.\\nYuan, W., Pang, R. Y ., Cho, K., Sukhbaatar, S., Xu, J.,\\nand Weston, J. Self-rewarding language models. arXiv\\npreprint arXiv:2401.10020 , 2024.\\n10', metadata={'source': './paper.pdf', 'page': 9}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 8: Function list of each type of agent.∗: descriptions of input, output and functionality.\\nType Function Name Functionality Input Output\\nMeta Agentcreate agent category level Create a category agent. Category name Category agent\\ngettools incategory Get tool names under a category. Category name [Tool names]\\ngettool descriptions Get description of each tool. [Tools] [Tool descriptions]\\nfinish search Send out finish signal. None None\\nCategory Agentcreate agent tool level Create a tool agent. [Tools] Tool agent\\ngettools incategory Get tool names under a category. Category name [Tool names]\\ngettool descriptions Get description of each tool. [Tools] [Tool descriptions]\\nfinish search Send out finish signal. None None\\nTool AgentaddAPIinto APIpool Add APIs into candidate pool. [APIs] None\\ngetAPIs intool Get API names under a tool. Tool name [API names]\\ngetAPIdetail Get detail∗of each API. [API names] [API details]\\ncheck ifrequest solvableCheck whether the query is solv-\\nable using the current candidate\\npool.None True \\\\False\\nfinish search Send out finish signal. None None\\nTable 9: Results on the original ToolBench (Qin et al., 2023b). Note that the original ToolBench includes non-solvable\\nqueries. We use pass rate defined in Eq 2 and illustrated in Figure 4(b), as the metric. All results are reproduced. Ref.:\\nreference; Avg.: average; SR: self-reflective.\\nModel API Retriever SolverUse Ref.\\nAPIsG1 G2 G3\\nAvg. (%)I (%) T (%) C (%) I (%) C (%) I (%)\\nToolLLM ToolLLM’s ToolLLaMA w/ DFSDT 24.0 23.0 37.5 17.5 16.5 4.0 20.4\\nToolLLM ToolLLM’s GPT-4 w/ DFSDT 32.0 43.5 46.5 30.0 33.0 8.0 32.2\\nAnyTool (Ours) SR Agent SR GPT-4 w/ DFSDT 46.0 54.0 53.0 37.0 46.5 32.0 44.8\\nA. More Implementation Details and Experimental Results\\nA.1. More Implementation Details of AnyTool\\nFor the solver implementing DFSDT, we set the maximum number of API calls to 10. Additionally, for our AnyTool, we\\nestablish a limit of 200,000 tokens for efficiency. This limit encompasses the token consumption by various components,\\nincluding the meta-agent, the tool agents, the category agents, the solver, and the self-reflection mechanism.\\nA.2. Detailed Function List\\nWe provide the function list of each type of agent in Table 8.\\nA.3. Results on the Original ToolBench\\nWe also provide the results on the original ToolBench (Qin et al., 2023b) without undergoing filtering process. In the\\noriginal ToolBench, each subset comprises 200 queries, except for G3-I, which contains 100 queries. Note that the original\\nToolBench includes non-solvable queries. We test all queries, regardless of whether they are solvable or not, using pass rate\\ndefined in Eq 2 and illustrated in Figure 4(b), as the metric. All results are reproduced. As shown in Table 9, our AnyTool\\noutperforms all ToolLLM (Qin et al., 2023b) variants.\\nA.4. GPT-4 with Various Plain Agents\\nIn Table 1 of the main paper, we present a comparison between our AnyTool and a GPT-4 variant. This variant em-\\nploys a plain agent as the API retriever, which is limited to accessing only the names of tools and APIs. It utilizes the\\naddAPIinto APIpool function to incorporate APIs into the API candidate pool. When an API is added to the pool,\\nwe use the check ifrequest solvable function to determine whether the current API candidates are adequate for\\naddressing the query. If the evaluation returns “True”, the solver begins to resolve the query using the API candidates with\\nthe DFSDT algorithm. Note that the plain agent does not involve any self-reflection mechanism.\\n11', metadata={'source': './paper.pdf', 'page': 10}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 10: Comparison of AnyTool and GPT-4 using various plain agents as the API retriever. The only difference among\\nthese plain agents lies in the information they can access.\\nGPT-4 Variant G2-I (%) G3-I (%)\\nw/ Names 13.1 13.2\\nw/ Names+Description 15.9 13.2\\nw/ Names+Description+Details 13.1 13.2\\nAnyTool (Ours) 58.9 63.2\\nTable 11: Comparison of AnyTool and GPT-4 using various AutoGen-RAG agents as the API retriever. The only difference\\namong these AutoGen-RAG agents lies in the embedding model they use.\\nEmbedding Model G2-I (%) G3-I (%)\\ntext-embedding-ada-002 8.4 7.9\\nall-mpnet-base-v2 7.4 7.9\\nAnyTool (Ours) 58.9 63.2\\nIn Table 10, we explore alternative configurations where the plain agent could access both names anddetailed descriptions\\nof tools and APIs (every 100 APIs a group), or even comprehensive information including the names ,descriptions , and\\nspecific API details (every 50 APIs a group). Our findings suggest that the addition of more detailed information leads\\nto only marginal improvements in performance. In contrast, our AnyTool exhibits superior performance, which can be\\nattributed to its hierarchical structure.\\nA.5. GPT-4 with Various AutoGen-RAG Agents\\nRetrieval-augmented generation (RAG) operates by receiving an input and sourcing a collection of pertinent or corroborative\\ndocuments from a reference, such as Wikipedia. These documents are then combined with the initial input prompt to provide\\ncontext. This enriched input is subsequently processed by LLMs to generate the final output. The RAG method enhances\\nthe performance of LLMs in situations that require accurate factual information.\\nIn Table 1 of the main paper, we present a version of GPT-4 designed for tool utilization. This version employs AutoGen-\\nRAG as the API retriever. The embedding model, known as “all-mpnet-base-v2”3, is utilized in this version. Specifically,\\nwe integrate the category names, tool names, API names, and their descriptions into a document, which is then divided into\\nnumerous text segments, each containing 1,000 tokens. Then, given a user query, AutoGen-RAG identifies the most relevant\\nsegments based on the embedding similarities between the user query and each text segment. Finally, we use GPT-4 to\\nextract the most relevant API candidates from the selected text segments.\\nWe provide another variant, where OpenAI’s “text-embedding-ada-002” is used as the embedding model. The comparison\\nwith our AnyTool is shown in Table 11.\\nA.6. Consumption Analysis\\nIn our analysis of resource consumption by AnyTool for solving queries across all datasets, we find that, on average,\\neach query consumes 13.5×104tokens, identifies 14.1 API candidates, and involves 43.3 OpenAI API calls and 4.6\\nself-reflections. Table 12 presents the statistics for each dataset. Additionally, calculating the processing time for resolving\\nqueries with AnyTool is infeasible. AnyTool relies on the function-calling feature of GPT-4, whose server response is often\\nunstable.\\nA.7. Filtering Process for ToolBench\\nWe primarily screen out non-solvable queries in ToolBench based on the following principles:\\n•Queries lacking essential information, such as unspecified phone numbers or ambiguous references like “my friend”.\\n3https://huggingface.co/sentence-transformers/all-mpnet-base-v2\\n12', metadata={'source': './paper.pdf', 'page': 11}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 12: Consumption statistics for each dataset.\\nStatisticsG1 G2 G3\\nATB Avg.I T C I C I\\nAverage Token Consumption ( ×104) 13.6 12.1 8.5 17.7 14.8 16.2 12.2 13.6\\nAverage Call Number 39.3 38.8 33.8 54.0 57.6 35.7 44.2 43.3\\nAverage Self-Reflection Number 4.2 3.8 4.1 5.7 5.2 5.1 4.0 4.6\\nAverage API Candidate Number 13.8 13.0 7.7 16.8 16.0 16.3 14.9 14.1\\nTable 13: Examples of our AnyToolBench.\\nI am creating an art project about the influence of music on visual arts and for my centerpiece, I would love to have an\\nAI-generated image based on the current number one hit song on the Billboard Hot 100 chart. Could you provide me\\nwith such an image that encapsulates the essence of the song ’Bad Habit’ by Steve Lacy?\\nFor a business presentation on global trends in music and sports performance analysis, could you provide the top\\nstreaming songs on Spotify for the most recent available global chart data, along with the corresponding ’hello world’\\nplaceholder text that will be used for introducing programmatic greetings, and the win-loss records for NFL teams from\\nthe 2022 season to illustrate the competitive landscape?\\nCould you analyze potential profit or loss from bitcoin arbitrage among exchanges, considering the market order fees,\\nand check if the IP 23.129.64.215 is flagged for any suspicious activity, and why? I’m interested in arbitrage between\\nBitfinex, Kraken, and Bittrex for BTC/USD and knowing what risks I might face using the mentioned IP address for\\ntransactions.\\nI plan to improve my daily fitness level, but I always lack proper planning. My current weight is 70 kilograms and\\nmy height is 1.75 meters. Given this, could you provide me a health plan regarding the weather condition for outdoor\\nactivities in New York for the next five days and the nutrition I intake by usually eating salad?\\nThese are inherently non-solvable since APIs require explicit input parameters.\\n• Queries containing fake parameters, such as non-existent URLs.\\n•Queries that specify a specific API are filtered out because they do not represent realistic scenarios. Moreover, if the\\nproblem can be solved using another API, it is difficult to determine whether it counts as a resolution.\\n•Unreasonable queries, such as asking for information about popular movies on YTS, which are too broad in scope and\\ndifficult to evaluate.\\nA.8. Construction of AnyToolBench\\nWe provide GPT-4 with several functions to freely explore the entire API pool, including {gettools incategory ,\\ngettool descriptions ,getAPIs intool ,getAPIdetail }. The functionality of these functions are listed\\nin Table 8. GPT-4 then utilizes the addAPIinto APIpool function to incorporate the selected APIs into an API\\ncandidate pool. Following this step, GPT-4 generates the required parameters for these APIs and formulates queries based\\non the actual responses from these APIs. We also prompt GPT-4 to generate a solution for each query, which significantly\\nreduces the potential for hallucinations—the queries may be formulated without utilizing the APIs. Moreover, we enhance\\nthe quality of these queries by verifying that the provided reference solutions truly resolve the queries. This rigorous process\\nensures that every query in our dataset is solvable. The prompt for constructing AnyToolBench is detailed in Section B.4.\\nWe show some examples of our AnyToolBench in Table 13.\\nB. Prompts\\nB.1. Bootstrap Prompt for the API Retriever\\nThe API retriever is composed of a meta-agent along with several category agents and tool agents. The bootstrap prompts\\nfor these three types of agents are presented in Table 14, Table 15, and Table 16, respectively.\\n13', metadata={'source': './paper.pdf', 'page': 12}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 14: Bootstrap prompt for meta-agent.\\nYou are APIGPT, with access to a database of APIs. This database is organized\\ninto the following categories: {categories }. Your task is to help users\\nidentify the relevant categories for their needs. To do this, you can\\nuse the ’query tools incategory’ function to retrieve the available tools\\nwithin a specific category. If you are unsure about the functionality of\\nsome tools, the ’get tools descriptions’ function can be used to obtain\\ndetailed information about these tools. This information will aid you in\\nunderstanding the general functionality of each category. Additionally, the\\n’create agent category level’ function allows you to assign a relevant category\\nto an agent, with each agent being assigned only one category. However,\\nyou can assign multiple categories to different agents. It is important\\nto explore as many categories as possible, as the solution to a query may\\nbe found in unexpected categories. Remember, your goal is not to answer\\nthe query directly but to identify all potentially relevant categories and\\nassign them to agents. Once you have completed the assignment, call the\\n’Finish’ function. At each step, you should briefly analyze the current\\nstatus and determine your next action, including the function calls needed to\\nexecute your step. Keep your analysis concise, ideally no longer than three\\nsentences.\\nTable 15: Bootstrap prompt for category agent.\\nYou are APIGPT, with access to a database of APIs categorized into various\\ngroups. Each category contains numerous tools, and each tool encompasses\\nmultiple APIs. Your task is to assist users in finding relevant tools within\\na specific category. If uncertain about the functionality of some tools, use\\nthe ’get tools descriptions’ function to obtain detailed information. Then,\\nemploy the ’create agent tool level’ function to allocate a subset of pertinent\\ntools to an agent, ensuring that similar tools are assigned to the same agent\\nand limiting the allocation to no more than five tools per agent. You may\\nassign different subsets to multiple agents. Remember, your role is not to\\nanswer queries directly, but to assign all possible tools. Once you complete\\nthe assignment, or if you determine the query is irrelevant to the tools in\\nthe specified category, invoke the ’Finish’ function. Execute each step by\\ncalling the appropriate functions, and keep your thought process concise,\\nideally within three sentences.\\nTable 16: Bootstrap prompt for tool agent.\\nYou are APIGPT with access to a database of APIs, categorized into various\\nsections. Each category contains multiple tools, and each tool encompasses\\nnumerous APIs. Your task is to assist users in finding relevant APIs within\\nthe tools ’ {tools }’ of the ’ {category }’ category. You will be provided with\\ndescriptions and details of these tools and their APIs. Upon identifying\\nrelevant API names, use the ’add apis into apipool’ function to add them to\\nthe final API list. If you conclude that all possible APIs have been explored,\\nor if there are no relevant APIs in these tools, invoke the Finish function.\\nDuring the process, you may receive feedback on these APIs. At each step,\\nensure to execute your actions using the appropriate functions. Keep your\\nresponses concise, ideally within three sentences.\\nB.2. Bootstrap Prompt for the Solver\\nWe adapt the prompt from ToolLLM (Qin et al., 2023b) to include a “give up” option without restarting. Furthermore, we\\nprompt it to provide a reason when choosing either “give upandrestart” or “give up”. The reason should mention specific\\n14', metadata={'source': './paper.pdf', 'page': 13}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 17: Bootstrap prompt for the solver.\\nYou are AutoGPT, you can use many tools (functions) to do the following task.\\nFirst I will give you the task description, and your task start. At each step,\\nyou need to give your thought to analyze the status now and what to do next,\\nwith a function call to actually excute your step. After the call, you will\\nget the call result, and you are now in a new state. Then you will analyze\\nyour status now, then decide what to do next... After many (Thought-call)\\npairs, you finally perform the task, then you can give your finial answer. If\\nyou feel you cannot solve the task or can only solve it partially, you should\\nchoose to give up and give your reason which should mention the names of the\\nfailed functions. Remember: 1.the state change is irreversible, you can’t go\\nback to one of the former state, if you want to restart the task, say \"I give\\nup and restart\" and give the reason. 2.All the thought is short, at most in 5\\nsentence. 3.You can do more then one try, so if your plan is to continuously\\ntry some conditions, you can do one of the conditions per try. Let’s Begin!\\nTask description: {task description }\\nTable 18: Bootstrap prompt for re-activating tool agents.\\nThe current APIs have failed to solve the query, resulting in: {fail reason }.\\nYou need to analyze this result and seek additional APIs. It’s possible that\\nthe tools lack the relevant APIs. In such cases, you should call the Finish\\nfunction. Remember not to invent tool or API names.\\nTable 19: Bootstrap prompt for re-activating category agents.\\nThe current APIs have failed to solve the query, and the reason is:\\n{fail reason }. Please consider assigning more unexplored tools to the agents.\\nTable 20: Bootstrap prompt for re-activating meta-agent.\\nThe current APIs have failed to solve the query, and the reason is:\\n{fail reason }. Please consider assigning more unexplored categories to the\\nagents.\\nfunction names. Table 17 details the prompt for the DFSDT implementation. The task description includes descriptions of\\naccessible functions; therefore, it should be updated to reflect changes in the API candidate pool.\\nB.3. Bootstrap Prompt for the Self-Reflection Mechanism\\nSelf-reflection mechanism re-activates AnyTool sequentially, first activating the API retriever and then the solver. Owing to\\nthe hierarchical design of our API retriever, we systematically re-activate various agents, following an ascending order from\\ntool agents, to category agents, and finally to the meta-agent. The prompts for re-activating the tool agents, the category\\nagents and the meta-agent are presented in Table 18, Table 19, and Table 20, respectively.\\nB.4. Prompt for Creating AnyToolBench\\nThis can be found in Table 21.\\nC. Case Study\\nIn Figure 7, we present a case study that demonstrates the process of resolving a user query using AnyTool. The self-\\nreflection mechanism reactivates the tool, category, and the meta agents sequentially. It is worth noting that not all agents\\nare reactivated. Subsequently, the solver is reactivated to attempt addressing the user query again, utilizing the updated API\\ncandidate pool. This self-reflection mechanism can be employed multiple times until the termination criteria are met—either\\nthe query is regarded as solved by the evaluator, or the number of self-reflections reaches the maximum limit.\\n15', metadata={'source': './paper.pdf', 'page': 14}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nTable 21: Prompt for Creating AnyToolBench.\\nYour task is to interact with a sophisticated database of tools and functions,\\noften referred to as APIs, to construct a user query that will be answered\\nusing the capabilities of these APIs. This database is organized into various\\ncategories, indicated by {categories }. To guide your exploration and selection\\nof the appropriate APIs, the database offers several meta functions:\\nExploration Functions:\\n1. Use get tools incategory to explore tools in a specific category.\\n2. Employ get apis intool to discover the list of APIs available within a\\nselected tool.\\n3. If you need detailed information about a tool, get tool descriptions will\\nprovide it.\\n4. For in-depth understanding of an API’s functionality, turn to\\ngetapidetails.\\nSelection and Testing Functions:\\n1. As you identify relevant functions, add them to your working list using\\naddapis into apipool.\\n2. Test these functions by synthesizing and applying various parameters.\\nThis step is crucial to understand how these functions can be practically\\napplied in formulating your query.\\n3. Should you find any function obsolete or not fitting your query context,\\nremove them using remove apis from apipool.\\nQuery Formulation Guidelines:\\n1.Your formulated query should be comprehensive, integrating APIs from 2\\nto 5 different categories. This cross-functional approach is essential to\\ndemonstrate the versatility and broad applicability of the database.\\n2.Avoid using ambiguous terms. Instead, provide detailed, specific\\ninformation. For instance, if your query involves personal contact details,\\nuse provided placeholders like {email }for email, {phone number }for phone\\nnumber, and URLs like {url}for a company website.\\n3.The query should be relatable and understandable to users without requiring\\nknowledge of the specific tools or API names used in the background. It\\nshould reflect a real-world user scenario.\\n4. Aim for a query length of at least thirty words to ensure depth and\\ncomplexity.\\nFinal Steps:\\n1.Once you’ve crafted the query, use the Finish function to submit it along\\nwith the corresponding answer. The answer should be direct and concise,\\naddressing the query without delving into the operational plan of the APIs.\\n2.Remember, the total number of calls to the initial meta functions should not\\nexceed 20.\\n3.Consider various use cases while formulating your query, such as data\\nanalysis in business contexts or educational content in academic settings.\\nYour approach should be creative and inclusive, catering to users with\\ndifferent skill levels and cultural backgrounds. Ensure that the query is\\nglobally relevant and straightforward, serving a singular purpose without\\ndiverging into unrelated areas. The complexity of your query should stem from\\nthe synthesis of information from multiple APIs.\\n16', metadata={'source': './paper.pdf', 'page': 15}),\n",
       " Document(page_content='AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nQuery\\n“I\\'m organizing a charity event to raise awareness for animal rights. Can you recommend a book that highlights the \\nimportance of compassion towards animals? Additionally, provide me with a random word that symbolizes unity and \\nempathy.”\\nMeta -Agent\\n1.Call get_tools_descriptions  ([\\'GetBooksInfo \\', \\'Book \\nFinder\\', \\'Random Word\\', \\'Dictionary\\', \\'Master \\nDictionary\\', \\'Random Words\\', \\'Random Ukrainian \\nWord\\', \\'Random Words - Spanish and French\\'])\\n2.Call create_agent_tool_level([\\' GetBooksInfo \\', \\'Book \\nFinder\\']) →Create  Tool-Agent -1\\n3.Call create_agent_tool_level([\\'Random Word\\', \\n\\'Random Words\\', \\'Random Ukrainian Word\\', \\n\\'Random Words - Spanish and French’]) → Create  \\nTool-Agent -2\\n4.Call create_agent_tool_level([\\'Dictionary\\', \\'Master \\nDictionary\\']) → Create  Tool-Agent -3\\n5.Call finish_search ()\\nCategory -Agent ( \\'Education \\')\\n Category -Agent ( \\'Text Analysis\\' )Bootstrap Prompt for the API Retriever \\n1.Call get_tools_in_category (\\'Education\\') \\n2.Call get_tools_in_category (\\'Other\\')                       \\n3.Call create_agent_category_level (‘Education’) →Create  Category -Agent ( \\'Education \\')  \\n4.Call get_tools_in_category (\\'Text Analysis\\')\\n5.Call create_agent_category_level (‘Text Analysis’)  →Create  Category -Agent ( \\'Text Analysis\\' )\\n6.Call finish_search ()\\n1.Call get_tools_descriptions  ([\\'Random Word by API -\\nNinjas\\')\\n2.Call get_tools_descriptions  ([\\'Synonyms Words\\'])\\n3.Call get_tools_descriptions  ([\\'Best Paraphrasing \\nAPI\\'])\\nTool-Agent -1\\n1.Call add_API_into_API_pool ([\\'Search\\',\\'Find  \\nbooks\\']) → Add APIs\\n2.Call check_if_request_solvable ()\\nTool-Agent -2\\n1.Call add_API_into_API_pool  ([\\'wordSearchEnglish \\', \\n\\'Word infos \\'])→ Add APIs\\n2.Call check_if_request_solvable ()\\nTool-Agent -3\\n1.Call add_API_into_API_pool  ([\\'run. cgi\\',\\'Get a \\nrandom word\\']) → Add APIs\\n2.Call check_if_request_solvable ()\\nAPI C andidate Pool\\n\\'Search\\'              \\'Find books’               \\' wordSearchEnglish \\'            \\'Word infos \\'          \\' run.cgi \\'           \\'Get a random word\\'                     \\nBootstrap Prompt for the S olver\\nSolver\\n“Here is a book that might interest you: \\'Animal Spy: Animal Welfare Behind Enemy Lines\\' by Terry Spamer  & Gordon \\nThorburn. However, due to the query limit, I couldn\\'t generate a random word that symbolizes unity and empathy. \\nApologies for the inconvenience.”\\nBootstrap Prompt for the S elf-Reflection Mechanism\\nTool-Agent -1,Tool-Agent -2, Tool -Agent -3,\\nCategory -Agent ( \\'Text_analysis \\')\\nSolver\\n\"Here are some book recommendations that highlight the importance of compassion towards animals:  \\'Animal Spy: \\nAnimal Welfare Behind Enemy Lines\\' by Terry Spamer , Gordon Thorburn. A book by Leslie Irvine on the effect of disasters \\non animals. The random word generated is \\'transcendental\\', signifying elevated thinking and broader understanding, which \\nmight be seen as relevant to unity and empathy in a broader sense.\"Updated API C andidate Pool\\n\\'Find books\\', \\'Get multiple random words’,  \\'Retrieve Response\\'Re-activate: \\nFigure 7: Illustration of a case study.\\n17', metadata={'source': './paper.pdf', 'page': 16})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\\nYu Du1 *Fangyun Wei2 *†Hongyang Zhang3\\n1Tsinghua University2Microsoft Research Asia3University of Waterloo\\nduyu20@mails.tsinghua.edu.cn fawe@microsoft.com hongyang.zhang@uwaterloo.ca\\n*Equal contribution†Corresponding author\\nAbstract\\nWe introduce AnyTool, a large language model\\nagent designed to revolutionize the utilization of\\na vast array of tools in addressing user queries.\\nWe utilize over 16,000 APIs from Rapid API,\\noperating under the assumption that a subset of\\nthese APIs could potentially resolve the queries.\\nAnyTool primarily incorporates three elements:\\nan API retriever with a hierarchical structure, a\\nsolver aimed at resolving user queries using a se-\\nlected set of API candidates, and a self-reflection\\nmechanism, which re-activates AnyTool if the ini-\\ntial solution proves impracticable. AnyTool is\\npowered by the function calling feature of GPT-4,\\neliminating the need for training external modules.\\nWe also revisit the evaluation protocol introduced\\nby previous works and identify a limitation in this\\nprotocol that leads to an artificially high pass rate.\\nBy revising the evaluation protocol to better re-\\nflect practical application scenarios, we introduce\\nan additional benchmark, termed AnyToolBench.\\nExperiments across various datasets demonstrate\\nthe superiority of our AnyTool over strong base-\\nlines such as ToolLLM and a GPT-4 variant tai-\\nlored for tool utilization. For instance, AnyTool\\noutperforms ToolLLM by +35.4% in terms of\\naverage pass rate on ToolBench. Code will be\\navailable at https://github.com/dyabel/AnyTool.\\n1. Introduction\\nFrom the dawn of civilization, humanity has embarked on\\na relentless journey of discovery and innovation, mastering\\nan ever-expanding array of tools to enhance our capabilities\\nand increase production efficiency. As we have evolved,\\nso have our tools, transitioning from simple stone imple-\\nments to complex machines and beyond. Today, we stand\\nat the forefront of a new era, reaping the benefits of the\\nrapid developments in artificial intelligence, particularly the\\nrecent advances in large language models (LLMs) (Brown\\net al., 2020; Touvron et al., 2023a;b; Chowdhery et al., 2023;\\nAchiam et al., 2023; Ouyang et al., 2022). A pivotal chal-\\nAPI Pool ( 16K+ APIs)\\n…AnyTool\\nSelf-Reflection SolutionQuery\\nAPI-Retriever\\nSolver\\n(a) AnyTool addresses user queries by leveraging 16k+ APIs.\\nIt integrates a hierarchical API-retriever, a solver, and a self-\\nreflection mechanism in a closed loop, all operating without\\nthe need for additional training.\\n(b) Comparison with ToolLLM and a GPT-4 variant tailored for\\ntool utilization across six subsets of ToolBench (Qin et al., 2023b),\\nusing pass rate defined in Eq 2 as the evaluation metric.\\nFigure 1: (a) Illustration of AnyTool. (b) Comparison in\\nperformance.\\nlenge now is learning how to drive LLMs to effectively use\\ntools (Qin et al., 2023a; Xu et al., 2023; Cai et al., 2023;\\nSong et al., 2023; Ruan et al., 2023; Shen et al., 2023; Hao\\net al., 2023), a task that could redefine our interaction with\\ntechnology. Towards this end, we introduce AnyTool, a\\nGPT-4-empowered agent, as depicted in Figure 1a. It is\\ndesigned to effectively leverage more than 16,000 APIs to\\n1arXiv:2402.04253v1  [cs.CL]  6 Feb 2024'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Yu Du1 *Fangyun Wei2 *†Hongyang Zhang3\n",
      "1Tsinghua University2Microsoft Research Asia3University of Waterloo\n",
      "duyu20@mails.tsinghua.edu.cn fawe@microsoft.com hongyang.zhang@uwaterloo.ca\n",
      "*Equal contribution†Corresponding author\n",
      "Abstract\n",
      "We introduce AnyTool, a large language model\n",
      "agent designed to revolutionize the utilization of\n",
      "a vast array of tools in addressing user queries.\n",
      "We utilize over 16,000 APIs from Rapid API,\n",
      "operating under the assumption that a subset of\n",
      "these APIs could potentially resolve the queries.\n",
      "AnyTool primarily incorporates three elements:\n",
      "an API retriever with a hierarchical structure, a\n",
      "solver aimed at resolving user queries using a se-\n",
      "lected set of API candidates, and a self-reflection\n",
      "mechanism, which re-activates AnyTool if the ini-\n",
      "tial solution proves impracticable. AnyTool is\n",
      "powered by the function calling feature of GPT-4,\n",
      "eliminating the need for training external modules.\n",
      "We also revisit the evaluation protocol introduced\n",
      "by previous works and identify a limitation in this\n",
      "protocol that leads to an artificially high pass rate.\n",
      "By revising the evaluation protocol to better re-\n",
      "flect practical application scenarios, we introduce\n",
      "an additional benchmark, termed AnyToolBench.\n",
      "Experiments across various datasets demonstrate\n",
      "the superiority of our AnyTool over strong base-\n",
      "lines such as ToolLLM and a GPT-4 variant tai-\n",
      "lored for tool utilization. For instance, AnyTool\n",
      "outperforms ToolLLM by +35.4% in terms of\n",
      "average pass rate on ToolBench. Code will be\n",
      "available at https://github.com/dyabel/AnyTool.\n",
      "1. Introduction\n",
      "From the dawn of civilization, humanity has embarked on\n",
      "a relentless journey of discovery and innovation, mastering\n",
      "an ever-expanding array of tools to enhance our capabilities\n",
      "and increase production efficiency. As we have evolved,\n",
      "so have our tools, transitioning from simple stone imple-\n",
      "ments to complex machines and beyond. Today, we stand\n",
      "at the forefront of a new era, reaping the benefits of the\n",
      "rapid developments in artificial intelligence, particularly the\n",
      "recent advances in large language models (LLMs) (Brown\n",
      "et al., 2020; Touvron et al., 2023a;b; Chowdhery et al., 2023;\n",
      "Achiam et al., 2023; Ouyang et al., 2022). A pivotal chal-\n",
      "API Pool ( 16K+ APIs)\n",
      "…AnyTool\n",
      "Self-Reflection SolutionQuery\n",
      "API-Retriever\n",
      "Solver\n",
      "(a) AnyTool addresses user queries by leveraging 16k+ APIs.\n",
      "It integrates a hierarchical API-retriever, a solver, and a self-\n",
      "reflection mechanism in a closed loop, all operating without\n",
      "the need for additional training.\n",
      "(b) Comparison with ToolLLM and a GPT-4 variant tailored for\n",
      "tool utilization across six subsets of ToolBench (Qin et al., 2023b),\n",
      "using pass rate defined in Eq 2 as the evaluation metric.\n",
      "Figure 1: (a) Illustration of AnyTool. (b) Comparison in\n",
      "performance.\n",
      "lenge now is learning how to drive LLMs to effectively use\n",
      "tools (Qin et al., 2023a; Xu et al., 2023; Cai et al., 2023;\n",
      "Song et al., 2023; Ruan et al., 2023; Shen et al., 2023; Hao\n",
      "et al., 2023), a task that could redefine our interaction with\n",
      "technology. Towards this end, we introduce AnyTool, a\n",
      "GPT-4-empowered agent, as depicted in Figure 1a. It is\n",
      "designed to effectively leverage more than 16,000 APIs to\n",
      "1arXiv:2402.04253v1  [cs.CL]  6 Feb 2024\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Meta-AgentCategory\n",
      "API-Candidate P oolFunction L ist\n",
      "create _agent_category_level (category_name )\n",
      "…Query\n",
      "get_tools_in_category (category_name )\n",
      "get_tool_descriptions ([tool_list ])\n",
      "finish_search ( )\n",
      "get_tools _in_category (category_name )\n",
      "get_tool_descriptions ([tool_list ])\n",
      "Sports Music\n",
      " Finance…\n",
      "Tool-1Tool-2Tool-3 Tool-M…\n",
      "API-1API-2API-3 API-D\n",
      "add_API_into_API_pool ([API_name_list ])\n",
      "get_API_details (API_name )\n",
      "check_if_request_solvable ( )Tool API\n",
      " API-2\n",
      " API-1\n",
      " API-D\n",
      " ; …\n",
      "Structure of 16K+ APIs:Category Agent\n",
      ":Tool Agent\n",
      "Self-Reflection Solved\n",
      " Solution\n",
      " Solver Unsolved\n",
      "…finish_search ( )finish_search ( )\n",
      "…\n",
      "orAPI R etriever\n",
      "create _agent_tool_level  ([tool_list ])\n",
      "get_APIs _in_tool(tool_name )\n",
      "Figure 2: Overview of AnyTool. It primarily consists of a hierarchical API retriever tasked with identifying the most\n",
      "relevant API candidates to the user query from a large API pool, a solver aimed at addressing the queries using the generated\n",
      "API-candidate pool, and a self-reflection mechanism. The hierarchical structure includes a meta-agent linked with several\n",
      "category agents, each of which manages a collection of tool agents. We leverage the API structure defined by Rapid API as\n",
      "a guideline. Each type of agent is assigned several functions that it can use to explore the API space. Refer to Table 8 in the\n",
      "appendix for the details of each function.\n",
      "address user queries, with a significant performance leap as\n",
      "depicted in Figure 1b.\n",
      "Previous research (Qin et al., 2023b) formulated tool uti-\n",
      "lization in a dual-phase approach: initially retrieving, then\n",
      "resolving. Specifically, the first phase involves retrieving the\n",
      "most pertinent APIs from a substantial collection of 16K+\n",
      "APIs in response to user queries. The subsequent phase fo-\n",
      "cuses on utilizing these chosen APIs to address user queries.\n",
      "Our AnyTool uses this design principle while introducing\n",
      "four distinct characteristics (see Figure 2 for an overview):\n",
      "Plug-and-Play. Our AnyTool does not require the training\n",
      "of any modules, except for the function-calling feature of\n",
      "GPT-4 (Achiam et al., 2023). This aspect sets it apart from\n",
      "existing methods like ToolLLM, which necessitates training\n",
      "an API retriever capable of selecting a set of candidate APIs\n",
      "from the API pool (Qin et al., 2023b).\n",
      "Hierarchical Structure. To identify the most relevant APIs\n",
      "for user queries, we design a hierarchical structure within\n",
      "our API retriever. This structure is composed of three tiers,\n",
      "each containing one or multiple agents with diverse roles.\n",
      "This arrangement is inspired by the divide-and-conquer ap-\n",
      "proach. Additionally, we effectively incorporate the API\n",
      "categorization suggested by Rapid API into our hierarchical\n",
      "structure. Consequently, this significantly reduces the search\n",
      "scope for each agent and overcomes constraints related to\n",
      "the maximum context length in LLMs.\n",
      "Figure 3: The performance of our AnyTool on different\n",
      "datasets (each denoted by a curve) improves as the number\n",
      "of self-reflection rounds increases. ATB: AnyToolBench.\n",
      "Self-Reflection Mechanism. Our AnyTool is designed to\n",
      "address user queries through a process of initial attempt\n",
      "followed by reflection. Upon receiving a query, AnyTool\n",
      "suggests a solution, which is then evaluated for feasibility\n",
      "by GPT-4. In cases where the proposed solution is deemed\n",
      "impractical, AnyTool is re-activated, with the considera-\n",
      "tion of reasons for failure and relevant historical contexts.\n",
      "This mechanism significantly reduces the tendency to “over-\n",
      "search” for simpler queries, while also providing a more\n",
      "context-rich and in-depth search for complex queries. This\n",
      "closed-loop system enhances the efficiency and effective-\n",
      "ness of the query resolution process. Figure 3 shows how\n",
      "the pass rate improves w.r.t. the self-reflection rounds. With\n",
      "2\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "only 4-6 self-reflection iterations, the pass rate improves by\n",
      "up to 20% across all datasets.\n",
      "Evaluation for Realistic Scenarios. The evaluation frame-\n",
      "work presented in ToolBench (Qin et al., 2023b) commences\n",
      "with categorizing user queries as either solvable or non-\n",
      "solvable, employing a set of reference APIs. Following this,\n",
      "the solvable queries undergo further scrutiny to determine if\n",
      "they are successfully addressed or not. However, for those\n",
      "non-solvable queries, the evaluation system regards them as\n",
      "solved when calculating the pass rate, leading to an artifi-\n",
      "cially high pass rate. Our study delves into the intricacies of\n",
      "this evaluation methodology and proposes a revised protocol\n",
      "that better mirrors practical application scenarios.\n",
      "In addition to evaluation on ToolBench, we introduce an\n",
      "extra benchmark, termed AnyToolBench, to facilitate the\n",
      "application of our new evaluation protocol. Experimen-\n",
      "tally, AnyTool achieves state-of-the-art performance, sur-\n",
      "passing strong baselines such as ToolLLM and a version of\n",
      "GPT-4 specifically tailored for tool utilization across various\n",
      "datasets, as illustrated in Figure 1b.\n",
      "2. Related Works\n",
      "Tool Utilization in LLMs. Large language models (Rad-\n",
      "ford et al., 2018; 2019; Brown et al., 2020; Touvron et al.,\n",
      "2023a;b; Thoppilan et al., 2022) may commit factual errors\n",
      "when responding to queries, particularly struggling with pre-\n",
      "cise numbers and specific fields of expertise (Huang et al.,\n",
      "2023; Augenstein et al., 2023). Utilizing tools can help miti-\n",
      "gate this issue (Li et al., 2023; Qin et al., 2023b; Parisi et al.,\n",
      "2022; Tang et al., 2023; Hsieh et al., 2023; Schick et al.,\n",
      "2023). Previous work has involved using an API retriever\n",
      "to match relevant APIs from a large API pool based on the\n",
      "documents, employing either an pretrained text embedding\n",
      "model (Li et al., 2023; Patil et al., 2023) or one finetuned\n",
      "with curated API retrieval data (Qin et al., 2023b). How-\n",
      "ever, this approach typically suffers from low accuracy and\n",
      "may overlook the truly relevant APIs. Moreover, there is a\n",
      "lack of feedback mechanism in their retrieval, often leading\n",
      "to unsolved queries due to incorrect API candidates being\n",
      "provided. Our AnyTool fills this gap by directly using the\n",
      "GPT-4 as the API retriever with a hierarchical structure de-\n",
      "sign, and introduces the self-reflection mechanism into the\n",
      "whole process.\n",
      "Self-Reflection Mechanism in LLMs. Self-reflection is a\n",
      "featured ability of LLMs. It was first studied in the LLM\n",
      "alignment problems. Wang et al. (2022) considered the\n",
      "ability of GPT-3 to self-generate instructions for alignment\n",
      "finetuning. Without finetuning, Li et al. (2024) introduced\n",
      "an inference method, RAIN, that allows pre-trained LLMs to\n",
      "evaluate their own generation and use the evaluation results\n",
      "to guide rewind and generation for AI safety. Recently,\n",
      "Chen et al. (2024) proposed a self-play mechanism, where\n",
      "the LLM refines its capability by playing against instances ofitself. Yuan et al. (2024) proposed self-rewarding language\n",
      "models, where the language model itself is used via LLM-\n",
      "as-a-Judge prompting to provide its own rewards for the\n",
      "following DPO finetuning (Rafailov et al., 2023). On the\n",
      "other hand, some negative results on self-reflection were\n",
      "also investigated. For example, Huang et al. (2023) showed\n",
      "that GPT-3.5-Turbo and GPT-4 cannot self-correct reasoning\n",
      "yet. But whether GPT-4 can serve as a self-reflective agent\n",
      "for API calling remains an open problem in the existing\n",
      "literature.\n",
      "3. Preliminaries\n",
      "3.1. Function Calling\n",
      "Function calling is a core characteristic of GPT-4 (Achiam\n",
      "et al., 2023). Specifically, in response to a user’s query Q,\n",
      "the function calling system accesses a set of Mdistinct\n",
      "functions {Fi}M\n",
      "i=1. Each function Fihas the potential to\n",
      "solveQ, a part of Q, or may not be relevant to Qat all.\n",
      "The functionality of Fiis elaborated in a specific document\n",
      "that outlines its purpose, required and optional parameters\n",
      "along with their explanations, the types of output it gener-\n",
      "ates, and the interpretations of these outputs. Note that the\n",
      "function calling feature of GPT-4 does not require visibility\n",
      "into the detailed implementations of each function. It under-\n",
      "stands their intentions and functionalities through linguistic\n",
      "comprehension.\n",
      "The process of function calling involves: 1) the user inputs\n",
      "both the query Qand the function list {Fi}M\n",
      "i=1, alongside\n",
      "a designated “Finish Function” F∗, into GPT-4; 2) GPT-\n",
      "4 generates a function calling request for the user, with\n",
      "clear input parameters; 3) the user executes the specific\n",
      "function and provides the historical context and function\n",
      "response to GPT-4; 4) this cycle of steps two and three is\n",
      "repeated multiple times until GPT-4 activates the “Finish\n",
      "Function” F∗, signaling the resolution of query Q. Users\n",
      "have the option to either employ the output of F∗directly,\n",
      "or to gather the interim results generated during the function\n",
      "calling process, according to their specific goals or design.\n",
      "3.2. Problem Formulation and Evaluation\n",
      "Problem Formulation. The objective of this work is to de-\n",
      "velop a proficient agent capable of utilizing a vast collection\n",
      "of real-world APIs to address user queries. We use over 16K\n",
      "real-world APIs from the RapidAPI Hub, as collected in the\n",
      "ToolLLM (Qin et al., 2023b). These APIs are represented as\n",
      "{APIi}N\n",
      "i=1, forming our API pool. The effectiveness of the\n",
      "solutions generated by the agent is assessed using GPT-4.\n",
      "This evaluation involves processing both the user query Q\n",
      "and the proposed solution S, in accordance with established\n",
      "evaluation protocols and criteria, to ascertain the solution’s\n",
      "ability to adequately address the query. We have also con-\n",
      "ducted human evaluation and find a correlation as high as\n",
      "96.5% between GPT-4 and human evaluations.\n",
      "3\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Query\n",
      "API CandidatesAgent\n",
      "QuerySolution\n",
      "GPT-4+\n",
      "Solvable\n",
      "Non-solvable Solved Unsolved\n",
      "…\n",
      "API Pool ( 16K+ APIs)Agent\n",
      "GPT-4Solution\n",
      "Solved Unsolved\n",
      "GPT-4\n",
      "Pass Rate =\n",
      "+\n",
      "+ +Pass Rate =\n",
      "+\n",
      "(a) Evaluation Protocol ( Prior Work ). (b) Evaluation Protocol ( Ours ).+\n",
      "Figure 4: Illustration of the evaluation protocols used by: (a) ToolLLM (Qin et al., 2023b); and (b) ours. In (a), if the API\n",
      "retriever selects candidates completely unrelated to the user’s query, GPT-4 may classify all queries as “non-solvable”,\n",
      "leading to an artificially high pass rate, despite the queries remaining unsolved. In (b), we conduct a manual review of all\n",
      "queries and retain only those queries that can be resolved with specific APIs from the API pool for ToolBench.\n",
      "Evaluation Protocol. We first revisit the evaluation pro-\n",
      "tocol initially introduced by ToolLLM (Qin et al., 2023b).\n",
      "ToolLLM employs a dual-phase approach for utilizing vari-\n",
      "ous APIs. In the first phase, an API retriever is developed\n",
      "to select the most relevant API candidates from the API\n",
      "pool according to a user query Q. The second phase in-\n",
      "volves ToolLLaMA, a specialized agent that formulates a\n",
      "solution using the selected API candidates. Due to its dual-\n",
      "phase nature, ToolLLM’s evaluation is twofold. Initially,\n",
      "GPT-4 evaluates whether the selected API candidates can\n",
      "address the query Q, categorizing them as either “solvable”\n",
      "or “non-solvable”. If a query is deemed “solvable”, GPT-\n",
      "4 then assesses the effectiveness of the provided solution,\n",
      "classifying it as either “solved” or “unsolved”. Figure 4(a)\n",
      "illustrates how the pass rate Ris calculated:\n",
      "R=#(Non-solvable ) +#(Solved )\n",
      "#(Non-solvable ) +#(Solved ) +#(Unsolved ).(1)\n",
      "However, a significant flaw exists in this evaluation protocol.\n",
      "If the API retriever selects candidates completely unrelated\n",
      "to the user’s query, GPT-4 may classify all queries as “non-\n",
      "solvable”, leading to an artificially high pass rate, despite\n",
      "the queries remaining unsolved. Our experimental evidence\n",
      "confirms this issue, showing that when API candidates are\n",
      "randomly selected for each query, GPT-4 predominantly\n",
      "labels them as “non-solvable”, resulting in an inflated pass\n",
      "rate of 99.0% through the metric defined in Eq 1.\n",
      "To address the limitations inherent in ToolLLM’s evaluation\n",
      "protocol, we propose an alternative evaluation methodol-\n",
      "ogy that aligns more closely with real-world scenarios, as\n",
      "illustrated in Figure 4(b). Specifically, we bypass the first\n",
      "evaluation phase of ToolLLM, which assesses the potential\n",
      "of candidate APIs in addressing query Q. Instead, we di-\n",
      "rectly utilize GPT-4 to determine the efficacy of the agent’s\n",
      "proposed solution in resolving the query. The pass rate Risthus calculated using the formula:\n",
      "R=#(Solved )\n",
      "#(Solved ) +#(Unsolved ). (2)\n",
      "To ensure that all queries in the benchmark, namely Tool-\n",
      "Bench (Qin et al., 2023b), are solvable using certain APIs\n",
      "from the API pool, we conduct a manual review of all\n",
      "queries. We retain only those queries that can be resolved\n",
      "with specific APIs from this pool. The detailed process is\n",
      "available in Section A.7 of the appendix.\n",
      "4. AnyTool\n",
      "Our AnyTool exhibits several distinctive features: Firstly, it\n",
      "eliminates the need for training external modules, and solely\n",
      "relies on the function calling feature of GPT-4. Secondly,\n",
      "it can directly search the entire API pool, which contains\n",
      "over 16K APIs, using a hierarchical structure and a divide-\n",
      "and-conquer principle. Lastly, it is capable of self-reflection,\n",
      "enabling it to review and analyze unsolved user queries by\n",
      "taking into account reasons for failure and relevant historical\n",
      "contexts.\n",
      "Overview. The overview of AnyTool is depicted in Fig-\n",
      "ure 2. It primarily follows a three-step process to efficiently\n",
      "resolve the user query Q. The first step (Section 4.1) in-\n",
      "volves the creation of an API candidate pool. For efficiency,\n",
      "AnyTool is designed with a hierarchical architecture, taking\n",
      "advantage of the structured API organization available in\n",
      "Rapid API. In the second step (Section 4.2), a solver at-\n",
      "tempts to resolve query Qby utilizing these API candidates.\n",
      "Finally, if the query remains unsolved, AnyTool engages\n",
      "in a self-reflection process (Section 4.3) in an attempt to\n",
      "resolve it. A case study is shown in Section C.\n",
      "4.1. API Retriever\n",
      "Structured API Organization in Rapid API. Rapid API\n",
      "employs a structured system to categorize its extensive col-\n",
      "lection of 16K+ APIs. Specifically, this organization is\n",
      "4\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "divided into three distinct tiers: the first tier is the category\n",
      "level, encompassing various domains such as “sports” and\n",
      "“finance”; the second tier, designated as the tool level, con-\n",
      "sists of tools that belong to specific categories; and the third\n",
      "tier focuses on individual APIs, with each API belonging\n",
      "to a specific tool, as illustrated in Figure 2. This hierarchi-\n",
      "cal arrangement serves as a foundational guideline in the\n",
      "development of our API retriever.\n",
      "Hierarchical Structure. As depicted in Figure 2, the struc-\n",
      "ture of our API retriever consists of three tiers. At the initial\n",
      "tier, a meta-agent exists, tasked with dynamically generat-\n",
      "ing a series of category agents in response to the user query\n",
      "Q. The intermediary tier is comprised of multiple category\n",
      "agents, each established by the meta-agent. These agents\n",
      "correspond to individual categories as defined by Rapid\n",
      "API, with their primary objective being to identify the most\n",
      "relevant tools for the query Qfrom their respective tool\n",
      "collections. Subsequently, these category agents initiate the\n",
      "creation of various tool agents. It is important to note that\n",
      "each tool agent may manage multiple tools, depending on\n",
      "the decisions made by the category agents. The goal of each\n",
      "tool agent is to search through its managed APIs for those\n",
      "that might solve the query Q, and then add these APIs to an\n",
      "API-candidate pool. Each type of agent possesses its own\n",
      "distinct set of functions. These are illustrated in Figure 2\n",
      "and further detailed in Table 8 in the appendix.\n",
      "Generation of API-Candidate Pool. AnyTool is initiated\n",
      "upon receiving a query Q, the function list detailed in Ta-\n",
      "ble 8, and a bootstrap prompt as outlined in Section B.1 of\n",
      "the appendix. This process heavily relies on the function\n",
      "calling feature of GPT-4 (refer to Section 3.1). Operating\n",
      "interactively, our system enables agents (starting with the\n",
      "meta-agent) to send requests for calling their managed func-\n",
      "tions. These functions may involve creating a specific agent\n",
      "(either a category agent or a tool agent) or executing a par-\n",
      "ticular function, in accordance with the historical context.1\n",
      "The requests are parsed, and the corresponding functions\n",
      "are executed. The results produced by these functions are\n",
      "subsequently incorporated into the historical context, which\n",
      "is then returned to the agents. This process repeats contin-\n",
      "uously until the termination criteria are met. All agents,\n",
      "including meta-agents, category agents, and tool agents,\n",
      "operate independently in a multi-threaded manner, signifi-\n",
      "cantly accelerating the process. We maintain a global API\n",
      "candidate pool, allowing each tool agent to add APIs to\n",
      "this pool, using the function “ addAPIinto APIpool ”\n",
      "(refer to Figure 2 and Table 8). All agents cease\n",
      "operations only when a tool agent calls the function\n",
      "“check ifrequest solvable ” and receives a return\n",
      "value of “True”. Subsequently, an API-candidate pool is ob-\n",
      "tained. In addition, we record the historical context and sta-\n",
      "1Each agent, whether it is a meta-agent, category agent, or tool\n",
      "agent, maintains its own historical context independently.tus of each agent. An agent’s status is marked as “Finished”\n",
      "only if it calls the function “ finish search ” during the\n",
      "process. Agents marked as “Finished” are excluded in the\n",
      "self-reflection process, which will be described later.\n",
      "4.2. Solver\n",
      "Functionality. The primary goal of the solver is to ad-\n",
      "dress the user’s query Q, utilizing the generated API candi-\n",
      "date pool. It is implemented as a singular agent that lever-\n",
      "ages the function-calling capabilities inherent in GPT-4.\n",
      "Two potential implementations for the solver are the Depth-\n",
      "First Search-Based Decision Tree (DFSDT) or the Chain\n",
      "of Thought (CoT) approach. A concise overview of the\n",
      "process is provided, with comprehensive details available\n",
      "in ToolLLM (Qin et al., 2023b). The solver activates upon\n",
      "receiving a query Q, in conjunction with a suite of func-\n",
      "tions, which includes those from the API candidate pool and\n",
      "a distinctive function named “ finish ”, as well as a boot-\n",
      "strap prompt detailed in Section B.2 of the appendix. The\n",
      "“finish ” function yields one of three possible outcomes:\n",
      "“Give Solution”, “Try Backtrack”, or “Give Up”, with “Try\n",
      "Backtrack” being specific to the DFSDT implementation.\n",
      "Each iteration involves: 1) the solver sending a request to\n",
      "call a function, 2) the interpretation of this request and the\n",
      "execution of the function, and 3) the integration of the func-\n",
      "tion’s outcomes into the contextual history, which is then\n",
      "returned to the solver. This cycle continues until the solver\n",
      "gives a “Give Solution” or “Give Up” decision. Note that\n",
      "when the solver makes a “Give Up” decision, it is required\n",
      "to provide both the reason and the function name of the APIs\n",
      "that are irrelevant to the user’s query or do not work properly.\n",
      "Self-reflection mechanism is triggered under two scenarios:\n",
      "1) “Give Solution”, where GPT-4 reviews the solution and\n",
      "determines that the query remains unresolved, and 2) “Give\n",
      "Up”, where the solver fails to address the query.\n",
      "4.3. Self-Reflection Mechanism\n",
      "If the initial solution fails to resolve user queries, the self-\n",
      "reflection mechanism re-activates AnyTool sequentially, first\n",
      "activating the API retriever and then the solver. It is worth\n",
      "noting that this mechanism can be applied repeatedly until\n",
      "the termination condition is met.\n",
      "Self-Reflection in the API Retriever. Our self-reflection\n",
      "mechanism first identifies the reason why a user query re-\n",
      "mains unsolved. In instances where the solver opts to “Give\n",
      "Up”, the rationale provided by the solver is utilized. Con-\n",
      "versely, if the solver proposes a solution but GPT-4 assesses\n",
      "that it does not adequately address the query, the reasoning\n",
      "ascribed by GPT-4 is employed. Recall that we maintain a\n",
      "record of historical context for each agent within the API re-\n",
      "triever. We initially incorporate the identified reason into all\n",
      "these historical contexts. Owing to the hierarchical design\n",
      "of our API retriever, we systematically re-activate various\n",
      "agents for efficiency purposes, following an ascending order\n",
      "5\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 1: Main results on the filtered ToolBench. We use pass rate defined in Eq 2 and illustrated in Figure 4(b), as the metric.\n",
      "All results are reproduced. *: OpenAI’s text-embedding-ada-002; Ref.: reference; Avg.: average; SR: self-reflective.\n",
      "Model API Retriever SolverUse Ref.\n",
      "APIsG1 G2 G3\n",
      "Avg. (%)I (%) T (%) C (%) I (%) C (%) I (%)\n",
      "ToolLLM OpenAI TE∗ToolLLaMA w/ DFSDT 8.7 6.8 12.0 4.7 8.2 10.5 8.5\n",
      "ToolLLM ToolLLM’s ToolLLaMA w/ DFSDT 28.4 26.3 38.4 21.5 15.1 7.7 22.9\n",
      "ToolLLM ToolLLM’s GPT-4 w/ DFSDT 42.6 46.2 51.4 23.4 24.5 2.6 31.8\n",
      "ToolLLM None ToolLLaMA w/ DFSDT ✓ 29.4 31.8 37.1 19.6 22.4 13.2 25.6\n",
      "GPT-4 None GPT-4 w/ CoT ✓ 31.3 34.8 47.1 27.1 34.7 2.6 29.6\n",
      "GPT-4 None GPT-4 w/ DFSDT ✓ 36.5 49.2 51.4 38.3 39.8 18.4 38.9\n",
      "GPT-4 Plain Agent GPT-4 w/ DFSDT 13.9 23.5 17.6 13.9 9.2 13.2 15.2\n",
      "GPT-4 AutoGen-RAG GPT-4 w/ DFSDT 14.8 19.7 19.7 7.4 9.2 7.9 13.1\n",
      "GPT-3.5 None GPT-3.5 w/ CoT ✓ 37.5 37.1 42.9 24.3 22.4 5.3 28.3\n",
      "GPT-3.5 None GPT-3.5 w/ DFSDT ✓ 39.1 40.2 48.6 31.8 25.5 15.8 33.5\n",
      "AnyTool (Ours) SR Agent SR GPT-4 w/ DFSDT 52.2 61.4 67.6 58.9 45.9 63.2 58.2\n",
      "from tool agents, to category agents, and finally to the meta-\n",
      "agent. It is worth noting that only the agents not marked\n",
      "with a “Finished” status are re-activated. As a result, this\n",
      "process expands our API-candidate pool, incorporating new\n",
      "APIs that could potentially resolve the user’s query.\n",
      "Self-Reflection in the Solver. Recall that when the solver\n",
      "makes a “Give Up” decision, it is designed to identify the\n",
      "function names of the APIs that are irrelevant to the user’s\n",
      "query. For efficiency, we first remove these APIs from the\n",
      "expanded API-candidate pool and exclude items where these\n",
      "APIs are called from the historical context of the solver. The\n",
      "solver is then re-activated with a new bootstrap prompt (refer\n",
      "to Section B.3 in the appendix), the updated API-candidate\n",
      "pool, and the cleaned historical context. The remaining\n",
      "process is the same as described in Section 4.2.\n",
      "5. Experiments\n",
      "5.1. Setup\n",
      "Benchmarks. We conduct experiments on two benchmarks:\n",
      "1)ToolBench (Qin et al., 2023b); and 2) our own benchmark,\n",
      "termed AnyToolBench . ToolBench comprises six subsets:\n",
      "G1-Instruction (G1-I), G1-Tool (G1-T), G1-Category (G1-\n",
      "C), G2-Instruction (G2-I), G2-Category (G2-C), and G3-\n",
      "Instruction (G3-I). As described at the end of Section 3.2,\n",
      "we perform a manual review on ToolBench to exclude non-\n",
      "solvable queries. Details of this process can be found in\n",
      "Section A.7 of the appendix. After filtering, the remaining\n",
      "queries in these six subsets are 115, 132, 142, 107, 98, and\n",
      "38, respectively. Unless otherwise specific, we adopt the fil-\n",
      "tered ToolBench . Our benchmark, AnyToolBench , includes\n",
      "400 instances. The process of creating AnyToolBench is\n",
      "detailed in Section A.8 of the appendix.\n",
      "Evaluation Protocol. We employ the pass rate (as defined\n",
      "in Eq. 2) as our evaluation metric. To assess whether a\n",
      "solution generated by an agent can resolve the query, we\n",
      "use GPT-4-32K. The same prompt utilized in ToolBench is\n",
      "applied when GPT-4 serves as the judge.Alignment between GPT-4’s Decisions and Decisions\n",
      "Made by Human Evaluators. We conduct a compara-\n",
      "tive analysis between decisions made by human evaluators\n",
      "and those generated by GPT-4, focusing on samples from\n",
      "the G1-I subset of ToolBench. Specifically, for each query\n",
      "sample, AnyTool generates a solution, which is then as-\n",
      "sessed for its feasibility in addressing the query by both\n",
      "human evaluators and GPT-4. Our results reveal that GPT-\n",
      "4’s alignment with human evaluation stands at 96.5%, while\n",
      "that of GPT-3.5 is only 73.9%. Based on these findings, we\n",
      "exclusively utilize GPT-4 for our evaluations.\n",
      "5.2. Main Results\n",
      "We compare our AnyTool with the pioneering Tool-\n",
      "LLM (Qin et al., 2023b) and its variants, as well as various\n",
      "GPT-4 models tailored for tool utilization.\n",
      "ToolLLM and Its Variants. ToolLLM integrates an API\n",
      "retriever2and a solver designed to address user queries by\n",
      "employing API candidates produced by the retriever. The\n",
      "solver operates using a finely-tuned LLaMA model, named\n",
      "ToolLLaMA, and employs a depth-first search-based deci-\n",
      "sion tree (DFSDT) algorithm to resolve queries. For each\n",
      "query, ToolBench provides a set of reference APIs that are\n",
      "potentially relevant. These reference APIs offer a means to\n",
      "evaluate the solver’s effectiveness by allowing the bypassing\n",
      "of the API retriever step. It is worth noting that additional\n",
      "APIs from the complete API pool, containing over 16,000\n",
      "APIs, may also contribute to effectively resolving queries.\n",
      "Beyond the original ToolLLM, our experiments also ex-\n",
      "amine two variants: 1) one that substitutes ToolLLaMA\n",
      "with GPT-4 in the solver; 2) another that foregoes the API\n",
      "retriever and relies solely on reference APIs.\n",
      "Various GPT-4 Models. The function-calling feature of\n",
      "GPT-4 enables it to use APIs directly for resolving user\n",
      "queries. However, in our setting, we deal with over 16,000\n",
      "2ToolLLM’s API retriever is trained on pair-wise data. Each\n",
      "pair includes a user query and a set of APIs relevant to the query.\n",
      "6\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 2: Main results on our AnyToolBench. All models use\n",
      "DFSDT implementation in the solver. SR: self-reflective;\n",
      "PR: pass rate.\n",
      "Method API Retriever Solver PR (%)\n",
      "ToolLLM ToolLLM’s ToolLLaMA 18.9\n",
      "ToolLLM ToolLLM’s GPT-4 36.6\n",
      "GPT-4 Plain Agent GPT-4 14.0\n",
      "AnyTool (Ours) SR Agent SR GPT-4 73.8\n",
      "APIs. Integrating all these APIs—each with its unique\n",
      "function description, input, and output—into GPT-4 si-\n",
      "multaneously exceeds the maximum context length of the\n",
      "model, even for the version with the largest context length of\n",
      "128,000 tokens. Therefore, we compare four GPT-4 models:\n",
      "1) one that uses reference APIs and the Chain of Thought\n",
      "(CoT) (Wei et al., 2022) algorithm in the solver; 2) another\n",
      "that uses reference APIs and the DFSDT algorithm; 3) a\n",
      "third that employs a plain agent for API retrieval and in-\n",
      "corporates the DFSDT algorithm in the solver; 4) a fourth\n",
      "that leverages the Retrieval Augmented Generation (RAG)\n",
      "feature from AutoGen (Augenstein et al., 2023) for API\n",
      "retrieval, and uses the DFSDT algorithm to resolve user\n",
      "queries through the selected API candidates.\n",
      "In the implementation of GPT4-plain-agent, we divide the\n",
      "set of over 16K APIs into 33 groups, each containing 500\n",
      "APIs, with the exception of the 33rd group. These groups are\n",
      "then sequentially processed by GPT-4. The specific task as-\n",
      "signed to GPT-4 involves identifying the relevant APIs using\n",
      "theaddAPIinto APIpool function, which integrates\n",
      "them into the API-candidate pool. Refer to Section A.4 for\n",
      "more details. Information on AutogGen-RAG can be found\n",
      "in Section A.5.\n",
      "Main Results on ToolBench. In Table 1, we compare our\n",
      "AnyTool with various ToolLLM variants and GPT-4 models\n",
      "across six subsets of the filtered ToolBench dataset. The re-\n",
      "sults on the original ToolBench are available in Section A.3.\n",
      "Both the API retriever and the solver contribute to the final\n",
      "performance. The API retriever’s role is to efficiently iden-\n",
      "tify the most pertinent APIs from an extensive collection,\n",
      "while the solver is tasked with generating viable solutions\n",
      "for user queries. Instead of training an API retriever as\n",
      "ToolLLM does, we leverage the powerful function-calling\n",
      "feature of GPT-4 and overcome the challenge posed by its\n",
      "inherent maximum context length limitation, through the im-\n",
      "plementation of a hierarchical structure. Our self-reflection\n",
      "mechanism applies to both the API retriever and the solver,\n",
      "enabling the whole system to operate in a closed loop. Ow-\n",
      "ing to these factors, our AnyTool significantly outperforms\n",
      "both the original ToolLLM and GPT-4 using reference APIs,\n",
      "by +32.6 and +19.3 points, respectively, in terms of the\n",
      "average pass rate.\n",
      "Main Results on AnyToolBench. AnyToolBench evaluatesTable 3: Ablation study on the pass rate of main components.\n",
      "“-” and “+” symbols denote the removal and addition of a\n",
      "component from and into AnyTool, respectively.\n",
      "Configuration G2-I (%) G3-I (%)\n",
      "AnyTool 58.9 63.2\n",
      "-Hierarchical Structure 22.4 15.8\n",
      "-Self-Reflection 19.6 15.8\n",
      "-DFSDT/+CoT 50.5 60.3\n",
      "Table 4: Ablation study on the pass rate of self-reflection\n",
      "mechanism. All agents include the tool agents, the category\n",
      "agents and the meta-agent.\n",
      "Re-Activation G2-I (%) G3-I (%)\n",
      "Tool Agents 43.9 44.7\n",
      "Tool Agents + Category Agents 55.2 55.3\n",
      "All Agents 58.9 63.2\n",
      "an agent’s capability to resolve user queries utilizing the\n",
      "entire API pool. Consequently, an API retriever is essential\n",
      "in this setting. We do not supply reference APIs for each\n",
      "query; thus, making comparisons with counterparts lacking\n",
      "an API retriever is impractical. In Table 2, we compare\n",
      "our AnyTool with a top-performing ToolLLM variant and\n",
      "GPT-4, where a plain agent serves as the retriever. The\n",
      "consistent improvements demonstrated by AnyTool over\n",
      "these approaches affirm its effectiveness in a realistic setting.\n",
      "5.3. Ablation Studies\n",
      "Unless otherwise specific, all ablation studies are conducted\n",
      "on G2-I and G3-I of the filtered ToolBench.\n",
      "Effectiveness of the Main Elements. Our AnyTool com-\n",
      "prises two principal elements: firstly, an API retriever with a\n",
      "hierarchical structure, and secondly, a self-reflection mecha-\n",
      "nism. In Table 3, we examine three distinct configurations\n",
      "of AnyTool. These include: a) substituting our hierarchical\n",
      "API retriever with a flat-structure version, which merges the\n",
      "functions of agents at the category and tool levels (except for\n",
      "“agent creation” and “finish search” functions) into the func-\n",
      "tion list of the meta-agent; b) eliminating the self-reflection\n",
      "mechanism; and c) substituting the DFSDT algorithm with\n",
      "CoT, thereby disabling the backtracking feature in DFSDT.\n",
      "Our findings demonstrate significant positive effects of both\n",
      "the hierarchical structure and the self-reflection feature on\n",
      "AnyTool’s performance. Choosing CoT over DFSDT results\n",
      "in a decline in pass rates by 8.4 and 2.9, respectively.\n",
      "Self-Reflection Mechanism. In Section 4.3, we introduce\n",
      "a self-reflection mechanism that is first applied to the API\n",
      "retriever module. It re-activates various agents in ascending\n",
      "order, from tool agents to category agents, and finally to the\n",
      "meta-agent. In Table 4, we examine the different versions\n",
      "that reactivate distinct types of agents. Reactivating all\n",
      "agents results in the best performance, owing to the larger\n",
      "search space.\n",
      "Size of the API Pool. Users typically submit a wide range\n",
      "7\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 5: Study on the effects of the API pool’s size to the\n",
      "pass rate.\n",
      "Size of API Pool G2-I (%) G3-I (%)\n",
      "1,000 18.6 7.9\n",
      "5,000 26.3 23.7\n",
      "10,000 38.1 36.8\n",
      "All 58.9 63.2\n",
      "Table 6: Study on the maximal size of API-candidate pool.\n",
      "Maximal Size of API-Candidate Pool G2-I (%) G3-I (%)\n",
      "16 49.5 42.1\n",
      "32 58.9 55.3\n",
      "64 58.9 63.2\n",
      "Table 7: We study the maximum number of tools that a tool\n",
      "agent can manage in our API retriever.\n",
      "Maximum Number of Tools G2-I (%) G3-I (%)\n",
      "3 48.6 42.1\n",
      "5 58.9 57.9\n",
      "10 52.3 39.5\n",
      "of queries to the AI system, seeking solutions to real-world\n",
      "problems. To effectively address these queries, the sys-\n",
      "tem requires access to a diverse array of APIs. In general,\n",
      "a larger API pool is more likely to successfully resolve\n",
      "user queries, as it offers a higher probability of containing\n",
      "relevant APIs. This hypothesis is evaluated by randomly\n",
      "selecting subsets of APIs from the complete pool and using\n",
      "only these subsets to address user queries with our AnyTool.\n",
      "The results, presented in Table 5, support our hypothesis.\n",
      "Maximal Size of the API-Candidate Pool. AnyTool op-\n",
      "erates through a two-step process—the solver addresses\n",
      "queries by using an API-candidate pool, which is generated\n",
      "by our hierarchical API Retriever. One termination criterion\n",
      "for the API retriever is the fullness of this pool. We examine\n",
      "the impact of the maximal size of the API-candidate pool as\n",
      "shown in Table 6. We observe that a pool size of 64 nearly\n",
      "reaches saturation in terms of performance.\n",
      "Tool Agent in API retriever. Our API retriever is designed\n",
      "with a hierarchical structure, in which the tool agents at the\n",
      "bottom layer directly add APIs that may potentially address\n",
      "user queries, into the API-candidate pool. As described\n",
      "in Section 4.1, a tool agent can manage a maximum of K\n",
      "tools existing in Rapid API. We examine the value of Kin\n",
      "Table 7. A trade-off is observed: managing too many tools\n",
      "(e.g.,K= 10 ) leads to a larger search space and may cause\n",
      "overlooking of relevant APIs, while managing too few tools\n",
      "(e.g.,K= 3) might result in lower recall.\n",
      "Statistics of Self-Reflection Frequency. In Figure 5, we\n",
      "report the average self-reflection frequency across all in-\n",
      "stances within each subset of the filtered ToolBench and\n",
      "our AnyToolBench. As described in Section 4.3, we re-\n",
      "activate various agents in ascending order. Consequently,\n",
      "the frequency of tool agents is much higher than that of\n",
      "Figure 5: Statistics of average self-reflection frequency.\n",
      "ATB: AnyToolBench.\n",
      "Figure 6: Statistics of average agent quantity.\n",
      "category agents and meta-agent. Additionally, calculating\n",
      "the processing time for resolving queries with AnyTool is\n",
      "infeasible. AnyTool relies on the function-calling feature of\n",
      "GPT-4, whose server response is often unstable.\n",
      "Agent Quantity in API Retriever. The API retriever of\n",
      "AnyTool is hierarchically structured. Depending on the na-\n",
      "ture of user queries, the meta-agent can dynamically create\n",
      "a varying number of category agents. This process is anal-\n",
      "ogous to the way category agents create tool agents. The\n",
      "average number of agents across all instances in each subset\n",
      "of the filtered ToolBench and our AnyToolBench is depicted\n",
      "in Figure 6.\n",
      "6. Conclusion\n",
      "In this work, we introduce AnyTool, an advanced agent capa-\n",
      "ble of harnessing 16K+ APIs to effectively handle realistic\n",
      "user inquiries. The core of AnyTool is a hierarchical API re-\n",
      "triever coupled with a solver. Additionally, it incorporates a\n",
      "unique self-reflection mechanism, enhancing its proficiency\n",
      "in responding to user queries. We also revise the prior\n",
      "evaluation protocol to better reflect real-world application\n",
      "scenarios. Rigorous experiments conducted on ToolBench\n",
      "and our AnyToolBench demonstrate our approach’s supe-\n",
      "riority over established models. Finally, we highlight two\n",
      "future research directions: 1) optimizing the organization\n",
      "of APIs for improved performance and efficiency; 2) devel-\n",
      "8\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "oping an advanced open-source LLM specifically for API\n",
      "utilization, which could facilitate local deployments.\n",
      "Impact Statements\n",
      "Although AnyTool significantly enhances the effectiveness\n",
      "of resolving user queries through various tools, its perfor-\n",
      "mance in extremely complex scenarios has not been verified,\n",
      "owing to the absence of appropriate datasets. Furthermore,\n",
      "as AnyTool relies on the function-calling feature of GPT-4,\n",
      "the capabilities of GPT-4 also affect the feasibility of the\n",
      "solutions it generates.\n",
      "References\n",
      "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n",
      "Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\n",
      "Anadkat, S., et al. GPT-4 technical report. arXiv preprint\n",
      "arXiv:2303.08774 , 2023.\n",
      "Augenstein, I., Baldwin, T., Cha, M., Chakraborty, T.,\n",
      "Ciampaglia, G. L., Corney, D., DiResta, R., Ferrara,\n",
      "E., Hale, S., Halevy, A., et al. Factuality challenges\n",
      "in the era of large language models. arXiv preprint\n",
      "arXiv:2310.05189 , 2023.\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\n",
      "Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n",
      "Askell, A., et al. Language models are few-shot learners.\n",
      "Advances in neural information processing systems , 33:\n",
      "1877–1901, 2020.\n",
      "Cai, T., Wang, X., Ma, T., Chen, X., and Zhou, D.\n",
      "Large language models as tool makers. arXiv preprint\n",
      "arXiv:2305.17126 , 2023.\n",
      "Chen, Z., Deng, Y ., Yuan, H., Ji, K., and Gu, Q. Self-play\n",
      "fine-tuning converts weak language models to strong lan-\n",
      "guage models. arXiv preprint arXiv:2401.01335 , 2024.\n",
      "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\n",
      "G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\n",
      "Gehrmann, S., et al. PaLM: Scaling language modeling\n",
      "with pathways. Journal of Machine Learning Research ,\n",
      "24(240):1–113, 2023.\n",
      "Hao, S., Liu, T., Wang, Z., and Hu, Z. ToolkenGPT: Aug-\n",
      "menting frozen language models with massive tools via\n",
      "tool embeddings. arXiv preprint arXiv:2305.11554 , 2023.\n",
      "Hsieh, C.-Y ., Chen, S.-A., Li, C.-L., Fujii, Y ., Ratner, A.,\n",
      "Lee, C.-Y ., Krishna, R., and Pfister, T. Tool documen-\n",
      "tation enables zero-shot tool-usage with large language\n",
      "models. arXiv preprint arXiv:2308.00675 , 2023.\n",
      "Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu,\n",
      "A. W., Song, X., and Zhou, D. Large language mod-els cannot self-correct reasoning yet. arXiv preprint\n",
      "arXiv:2310.01798 , 2023.\n",
      "Li, M., Zhao, Y ., Yu, B., Song, F., Li, H., Yu, H., Li, Z.,\n",
      "Huang, F., and Li, Y . API-Bank: A comprehensive bench-\n",
      "mark for tool-augmented LLMs. In Proceedings of the\n",
      "2023 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing , pp. 3102–3116, 2023.\n",
      "Li, Y ., Wei, F., Zhao, J., Zhang, C., and Zhang, H. RAIN:\n",
      "Your language models can align themselves without fine-\n",
      "tuning. In International Conference on Learning Repre-\n",
      "sentations , 2024.\n",
      "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\n",
      "Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\n",
      "et al. Training language models to follow instructions\n",
      "with human feedback. Advances in Neural Information\n",
      "Processing Systems , 35:27730–27744, 2022.\n",
      "Parisi, A., Zhao, Y ., and Fiedel, N. TALM: Tool augmented\n",
      "language models. arXiv preprint arXiv:2205.12255 ,\n",
      "2022.\n",
      "Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla:\n",
      "Large language model connected with massive apis. arXiv\n",
      "preprint arXiv:2305.15334 , 2023.\n",
      "Qin, Y ., Hu, S., Lin, Y ., Chen, W., Ding, N., Cui, G., Zeng,\n",
      "Z., Huang, Y ., Xiao, C., Han, C., et al. Tool learning with\n",
      "foundation models. arXiv preprint arXiv:2304.08354 ,\n",
      "2023a.\n",
      "Qin, Y ., Liang, S., Ye, Y ., Zhu, K., Yan, L., Lu, Y ., Lin, Y .,\n",
      "Cong, X., Tang, X., Qian, B., et al. ToolLLM: Facilitating\n",
      "large language models to master 16000+ real-world APIs.\n",
      "arXiv preprint arXiv:2307.16789 , 2023b.\n",
      "Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\n",
      "et al. Improving language understanding by generative\n",
      "pre-training. OpenAI , 2018.\n",
      "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\n",
      "Sutskever, I., et al. Language models are unsupervised\n",
      "multitask learners. OpenAI , 2019.\n",
      "Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\n",
      "C. D., and Finn, C. Direct preference optimization: Your\n",
      "language model is secretly a reward model. arXiv preprint\n",
      "arXiv:2305.18290 , 2023.\n",
      "Ruan, J., Chen, Y ., Zhang, B., Xu, Z., Bao, T., Du, G., Shi,\n",
      "S., Mao, H., Zeng, X., and Zhao, R. TPTU: Task planning\n",
      "and tool usage of large language model-based ai agents.\n",
      "arXiv preprint arXiv:2308.03427 , 2023.\n",
      "Schick, T., Dwivedi-Yu, J., Dess `ı, R., Raileanu, R., Lomeli,\n",
      "M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-\n",
      "former: Language models can teach themselves to use\n",
      "tools. arXiv preprint arXiv:2302.04761 , 2023.\n",
      "9\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Shen, Y ., Song, K., Tan, X., Li, D., Lu, W., and Zhuang,\n",
      "Y . HuggingGPT: Solving ai tasks with ChatGPT and its\n",
      "friends in huggingface. arXiv preprint arXiv:2303.17580 ,\n",
      "2023.\n",
      "Song, Y ., Xiong, W., Zhu, D., Li, C., Wang, K., Tian, Y .,\n",
      "and Li, S. RestGPT: Connecting large language models\n",
      "with real-world applications via RESTful APIs. arXiv\n",
      "preprint arXiv.2306.06624 , 2023.\n",
      "Tang, Q., Deng, Z., Lin, H., Han, X., Liang, Q., and\n",
      "Sun, L. ToolAlpaca: Generalized tool learning for lan-\n",
      "guage models with 3000 simulated cases. arXiv preprint\n",
      "arXiv:2306.05301 , 2023.\n",
      "Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\n",
      "shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\n",
      "Du, Y ., et al. LaMDA: Language models for dialog appli-\n",
      "cations. arXiv preprint arXiv:2201.08239 , 2022.\n",
      "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\n",
      "M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\n",
      "Azhar, F., et al. Llama: Open and efficient foundation lan-\n",
      "guage models. arXiv preprint arXiv:2302.13971 , 2023a.\n",
      "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\n",
      "A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\n",
      "Bhosale, S., et al. Llama 2: Open foundation and fine-\n",
      "tuned chat models. arXiv preprint arXiv:2307.09288 ,\n",
      "2023b.\n",
      "Wang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A.,\n",
      "Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning\n",
      "language model with self generated instructions. arXiv\n",
      "preprint arXiv:2212.10560 , 2022.\n",
      "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\n",
      "Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought\n",
      "prompting elicits reasoning in large language models.\n",
      "Advances in Neural Information Processing Systems , 35:\n",
      "24824–24837, 2022.\n",
      "Xu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang,\n",
      "J. On the tool manipulation capability of open-source\n",
      "large language models. arXiv preprint arXiv:2305.16504 ,\n",
      "2023.\n",
      "Yuan, W., Pang, R. Y ., Cho, K., Sukhbaatar, S., Xu, J.,\n",
      "and Weston, J. Self-rewarding language models. arXiv\n",
      "preprint arXiv:2401.10020 , 2024.\n",
      "10\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 8: Function list of each type of agent.∗: descriptions of input, output and functionality.\n",
      "Type Function Name Functionality Input Output\n",
      "Meta Agentcreate agent category level Create a category agent. Category name Category agent\n",
      "gettools incategory Get tool names under a category. Category name [Tool names]\n",
      "gettool descriptions Get description of each tool. [Tools] [Tool descriptions]\n",
      "finish search Send out finish signal. None None\n",
      "Category Agentcreate agent tool level Create a tool agent. [Tools] Tool agent\n",
      "gettools incategory Get tool names under a category. Category name [Tool names]\n",
      "gettool descriptions Get description of each tool. [Tools] [Tool descriptions]\n",
      "finish search Send out finish signal. None None\n",
      "Tool AgentaddAPIinto APIpool Add APIs into candidate pool. [APIs] None\n",
      "getAPIs intool Get API names under a tool. Tool name [API names]\n",
      "getAPIdetail Get detail∗of each API. [API names] [API details]\n",
      "check ifrequest solvableCheck whether the query is solv-\n",
      "able using the current candidate\n",
      "pool.None True \\False\n",
      "finish search Send out finish signal. None None\n",
      "Table 9: Results on the original ToolBench (Qin et al., 2023b). Note that the original ToolBench includes non-solvable\n",
      "queries. We use pass rate defined in Eq 2 and illustrated in Figure 4(b), as the metric. All results are reproduced. Ref.:\n",
      "reference; Avg.: average; SR: self-reflective.\n",
      "Model API Retriever SolverUse Ref.\n",
      "APIsG1 G2 G3\n",
      "Avg. (%)I (%) T (%) C (%) I (%) C (%) I (%)\n",
      "ToolLLM ToolLLM’s ToolLLaMA w/ DFSDT 24.0 23.0 37.5 17.5 16.5 4.0 20.4\n",
      "ToolLLM ToolLLM’s GPT-4 w/ DFSDT 32.0 43.5 46.5 30.0 33.0 8.0 32.2\n",
      "AnyTool (Ours) SR Agent SR GPT-4 w/ DFSDT 46.0 54.0 53.0 37.0 46.5 32.0 44.8\n",
      "A. More Implementation Details and Experimental Results\n",
      "A.1. More Implementation Details of AnyTool\n",
      "For the solver implementing DFSDT, we set the maximum number of API calls to 10. Additionally, for our AnyTool, we\n",
      "establish a limit of 200,000 tokens for efficiency. This limit encompasses the token consumption by various components,\n",
      "including the meta-agent, the tool agents, the category agents, the solver, and the self-reflection mechanism.\n",
      "A.2. Detailed Function List\n",
      "We provide the function list of each type of agent in Table 8.\n",
      "A.3. Results on the Original ToolBench\n",
      "We also provide the results on the original ToolBench (Qin et al., 2023b) without undergoing filtering process. In the\n",
      "original ToolBench, each subset comprises 200 queries, except for G3-I, which contains 100 queries. Note that the original\n",
      "ToolBench includes non-solvable queries. We test all queries, regardless of whether they are solvable or not, using pass rate\n",
      "defined in Eq 2 and illustrated in Figure 4(b), as the metric. All results are reproduced. As shown in Table 9, our AnyTool\n",
      "outperforms all ToolLLM (Qin et al., 2023b) variants.\n",
      "A.4. GPT-4 with Various Plain Agents\n",
      "In Table 1 of the main paper, we present a comparison between our AnyTool and a GPT-4 variant. This variant em-\n",
      "ploys a plain agent as the API retriever, which is limited to accessing only the names of tools and APIs. It utilizes the\n",
      "addAPIinto APIpool function to incorporate APIs into the API candidate pool. When an API is added to the pool,\n",
      "we use the check ifrequest solvable function to determine whether the current API candidates are adequate for\n",
      "addressing the query. If the evaluation returns “True”, the solver begins to resolve the query using the API candidates with\n",
      "the DFSDT algorithm. Note that the plain agent does not involve any self-reflection mechanism.\n",
      "11\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 10: Comparison of AnyTool and GPT-4 using various plain agents as the API retriever. The only difference among\n",
      "these plain agents lies in the information they can access.\n",
      "GPT-4 Variant G2-I (%) G3-I (%)\n",
      "w/ Names 13.1 13.2\n",
      "w/ Names+Description 15.9 13.2\n",
      "w/ Names+Description+Details 13.1 13.2\n",
      "AnyTool (Ours) 58.9 63.2\n",
      "Table 11: Comparison of AnyTool and GPT-4 using various AutoGen-RAG agents as the API retriever. The only difference\n",
      "among these AutoGen-RAG agents lies in the embedding model they use.\n",
      "Embedding Model G2-I (%) G3-I (%)\n",
      "text-embedding-ada-002 8.4 7.9\n",
      "all-mpnet-base-v2 7.4 7.9\n",
      "AnyTool (Ours) 58.9 63.2\n",
      "In Table 10, we explore alternative configurations where the plain agent could access both names anddetailed descriptions\n",
      "of tools and APIs (every 100 APIs a group), or even comprehensive information including the names ,descriptions , and\n",
      "specific API details (every 50 APIs a group). Our findings suggest that the addition of more detailed information leads\n",
      "to only marginal improvements in performance. In contrast, our AnyTool exhibits superior performance, which can be\n",
      "attributed to its hierarchical structure.\n",
      "A.5. GPT-4 with Various AutoGen-RAG Agents\n",
      "Retrieval-augmented generation (RAG) operates by receiving an input and sourcing a collection of pertinent or corroborative\n",
      "documents from a reference, such as Wikipedia. These documents are then combined with the initial input prompt to provide\n",
      "context. This enriched input is subsequently processed by LLMs to generate the final output. The RAG method enhances\n",
      "the performance of LLMs in situations that require accurate factual information.\n",
      "In Table 1 of the main paper, we present a version of GPT-4 designed for tool utilization. This version employs AutoGen-\n",
      "RAG as the API retriever. The embedding model, known as “all-mpnet-base-v2”3, is utilized in this version. Specifically,\n",
      "we integrate the category names, tool names, API names, and their descriptions into a document, which is then divided into\n",
      "numerous text segments, each containing 1,000 tokens. Then, given a user query, AutoGen-RAG identifies the most relevant\n",
      "segments based on the embedding similarities between the user query and each text segment. Finally, we use GPT-4 to\n",
      "extract the most relevant API candidates from the selected text segments.\n",
      "We provide another variant, where OpenAI’s “text-embedding-ada-002” is used as the embedding model. The comparison\n",
      "with our AnyTool is shown in Table 11.\n",
      "A.6. Consumption Analysis\n",
      "In our analysis of resource consumption by AnyTool for solving queries across all datasets, we find that, on average,\n",
      "each query consumes 13.5×104tokens, identifies 14.1 API candidates, and involves 43.3 OpenAI API calls and 4.6\n",
      "self-reflections. Table 12 presents the statistics for each dataset. Additionally, calculating the processing time for resolving\n",
      "queries with AnyTool is infeasible. AnyTool relies on the function-calling feature of GPT-4, whose server response is often\n",
      "unstable.\n",
      "A.7. Filtering Process for ToolBench\n",
      "We primarily screen out non-solvable queries in ToolBench based on the following principles:\n",
      "•Queries lacking essential information, such as unspecified phone numbers or ambiguous references like “my friend”.\n",
      "3https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
      "12\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 12: Consumption statistics for each dataset.\n",
      "StatisticsG1 G2 G3\n",
      "ATB Avg.I T C I C I\n",
      "Average Token Consumption ( ×104) 13.6 12.1 8.5 17.7 14.8 16.2 12.2 13.6\n",
      "Average Call Number 39.3 38.8 33.8 54.0 57.6 35.7 44.2 43.3\n",
      "Average Self-Reflection Number 4.2 3.8 4.1 5.7 5.2 5.1 4.0 4.6\n",
      "Average API Candidate Number 13.8 13.0 7.7 16.8 16.0 16.3 14.9 14.1\n",
      "Table 13: Examples of our AnyToolBench.\n",
      "I am creating an art project about the influence of music on visual arts and for my centerpiece, I would love to have an\n",
      "AI-generated image based on the current number one hit song on the Billboard Hot 100 chart. Could you provide me\n",
      "with such an image that encapsulates the essence of the song ’Bad Habit’ by Steve Lacy?\n",
      "For a business presentation on global trends in music and sports performance analysis, could you provide the top\n",
      "streaming songs on Spotify for the most recent available global chart data, along with the corresponding ’hello world’\n",
      "placeholder text that will be used for introducing programmatic greetings, and the win-loss records for NFL teams from\n",
      "the 2022 season to illustrate the competitive landscape?\n",
      "Could you analyze potential profit or loss from bitcoin arbitrage among exchanges, considering the market order fees,\n",
      "and check if the IP 23.129.64.215 is flagged for any suspicious activity, and why? I’m interested in arbitrage between\n",
      "Bitfinex, Kraken, and Bittrex for BTC/USD and knowing what risks I might face using the mentioned IP address for\n",
      "transactions.\n",
      "I plan to improve my daily fitness level, but I always lack proper planning. My current weight is 70 kilograms and\n",
      "my height is 1.75 meters. Given this, could you provide me a health plan regarding the weather condition for outdoor\n",
      "activities in New York for the next five days and the nutrition I intake by usually eating salad?\n",
      "These are inherently non-solvable since APIs require explicit input parameters.\n",
      "• Queries containing fake parameters, such as non-existent URLs.\n",
      "•Queries that specify a specific API are filtered out because they do not represent realistic scenarios. Moreover, if the\n",
      "problem can be solved using another API, it is difficult to determine whether it counts as a resolution.\n",
      "•Unreasonable queries, such as asking for information about popular movies on YTS, which are too broad in scope and\n",
      "difficult to evaluate.\n",
      "A.8. Construction of AnyToolBench\n",
      "We provide GPT-4 with several functions to freely explore the entire API pool, including {gettools incategory ,\n",
      "gettool descriptions ,getAPIs intool ,getAPIdetail }. The functionality of these functions are listed\n",
      "in Table 8. GPT-4 then utilizes the addAPIinto APIpool function to incorporate the selected APIs into an API\n",
      "candidate pool. Following this step, GPT-4 generates the required parameters for these APIs and formulates queries based\n",
      "on the actual responses from these APIs. We also prompt GPT-4 to generate a solution for each query, which significantly\n",
      "reduces the potential for hallucinations—the queries may be formulated without utilizing the APIs. Moreover, we enhance\n",
      "the quality of these queries by verifying that the provided reference solutions truly resolve the queries. This rigorous process\n",
      "ensures that every query in our dataset is solvable. The prompt for constructing AnyToolBench is detailed in Section B.4.\n",
      "We show some examples of our AnyToolBench in Table 13.\n",
      "B. Prompts\n",
      "B.1. Bootstrap Prompt for the API Retriever\n",
      "The API retriever is composed of a meta-agent along with several category agents and tool agents. The bootstrap prompts\n",
      "for these three types of agents are presented in Table 14, Table 15, and Table 16, respectively.\n",
      "13\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 14: Bootstrap prompt for meta-agent.\n",
      "You are APIGPT, with access to a database of APIs. This database is organized\n",
      "into the following categories: {categories }. Your task is to help users\n",
      "identify the relevant categories for their needs. To do this, you can\n",
      "use the ’query tools incategory’ function to retrieve the available tools\n",
      "within a specific category. If you are unsure about the functionality of\n",
      "some tools, the ’get tools descriptions’ function can be used to obtain\n",
      "detailed information about these tools. This information will aid you in\n",
      "understanding the general functionality of each category. Additionally, the\n",
      "’create agent category level’ function allows you to assign a relevant category\n",
      "to an agent, with each agent being assigned only one category. However,\n",
      "you can assign multiple categories to different agents. It is important\n",
      "to explore as many categories as possible, as the solution to a query may\n",
      "be found in unexpected categories. Remember, your goal is not to answer\n",
      "the query directly but to identify all potentially relevant categories and\n",
      "assign them to agents. Once you have completed the assignment, call the\n",
      "’Finish’ function. At each step, you should briefly analyze the current\n",
      "status and determine your next action, including the function calls needed to\n",
      "execute your step. Keep your analysis concise, ideally no longer than three\n",
      "sentences.\n",
      "Table 15: Bootstrap prompt for category agent.\n",
      "You are APIGPT, with access to a database of APIs categorized into various\n",
      "groups. Each category contains numerous tools, and each tool encompasses\n",
      "multiple APIs. Your task is to assist users in finding relevant tools within\n",
      "a specific category. If uncertain about the functionality of some tools, use\n",
      "the ’get tools descriptions’ function to obtain detailed information. Then,\n",
      "employ the ’create agent tool level’ function to allocate a subset of pertinent\n",
      "tools to an agent, ensuring that similar tools are assigned to the same agent\n",
      "and limiting the allocation to no more than five tools per agent. You may\n",
      "assign different subsets to multiple agents. Remember, your role is not to\n",
      "answer queries directly, but to assign all possible tools. Once you complete\n",
      "the assignment, or if you determine the query is irrelevant to the tools in\n",
      "the specified category, invoke the ’Finish’ function. Execute each step by\n",
      "calling the appropriate functions, and keep your thought process concise,\n",
      "ideally within three sentences.\n",
      "Table 16: Bootstrap prompt for tool agent.\n",
      "You are APIGPT with access to a database of APIs, categorized into various\n",
      "sections. Each category contains multiple tools, and each tool encompasses\n",
      "numerous APIs. Your task is to assist users in finding relevant APIs within\n",
      "the tools ’ {tools }’ of the ’ {category }’ category. You will be provided with\n",
      "descriptions and details of these tools and their APIs. Upon identifying\n",
      "relevant API names, use the ’add apis into apipool’ function to add them to\n",
      "the final API list. If you conclude that all possible APIs have been explored,\n",
      "or if there are no relevant APIs in these tools, invoke the Finish function.\n",
      "During the process, you may receive feedback on these APIs. At each step,\n",
      "ensure to execute your actions using the appropriate functions. Keep your\n",
      "responses concise, ideally within three sentences.\n",
      "B.2. Bootstrap Prompt for the Solver\n",
      "We adapt the prompt from ToolLLM (Qin et al., 2023b) to include a “give up” option without restarting. Furthermore, we\n",
      "prompt it to provide a reason when choosing either “give upandrestart” or “give up”. The reason should mention specific\n",
      "14\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 17: Bootstrap prompt for the solver.\n",
      "You are AutoGPT, you can use many tools (functions) to do the following task.\n",
      "First I will give you the task description, and your task start. At each step,\n",
      "you need to give your thought to analyze the status now and what to do next,\n",
      "with a function call to actually excute your step. After the call, you will\n",
      "get the call result, and you are now in a new state. Then you will analyze\n",
      "your status now, then decide what to do next... After many (Thought-call)\n",
      "pairs, you finally perform the task, then you can give your finial answer. If\n",
      "you feel you cannot solve the task or can only solve it partially, you should\n",
      "choose to give up and give your reason which should mention the names of the\n",
      "failed functions. Remember: 1.the state change is irreversible, you can’t go\n",
      "back to one of the former state, if you want to restart the task, say \"I give\n",
      "up and restart\" and give the reason. 2.All the thought is short, at most in 5\n",
      "sentence. 3.You can do more then one try, so if your plan is to continuously\n",
      "try some conditions, you can do one of the conditions per try. Let’s Begin!\n",
      "Task description: {task description }\n",
      "Table 18: Bootstrap prompt for re-activating tool agents.\n",
      "The current APIs have failed to solve the query, resulting in: {fail reason }.\n",
      "You need to analyze this result and seek additional APIs. It’s possible that\n",
      "the tools lack the relevant APIs. In such cases, you should call the Finish\n",
      "function. Remember not to invent tool or API names.\n",
      "Table 19: Bootstrap prompt for re-activating category agents.\n",
      "The current APIs have failed to solve the query, and the reason is:\n",
      "{fail reason }. Please consider assigning more unexplored tools to the agents.\n",
      "Table 20: Bootstrap prompt for re-activating meta-agent.\n",
      "The current APIs have failed to solve the query, and the reason is:\n",
      "{fail reason }. Please consider assigning more unexplored categories to the\n",
      "agents.\n",
      "function names. Table 17 details the prompt for the DFSDT implementation. The task description includes descriptions of\n",
      "accessible functions; therefore, it should be updated to reflect changes in the API candidate pool.\n",
      "B.3. Bootstrap Prompt for the Self-Reflection Mechanism\n",
      "Self-reflection mechanism re-activates AnyTool sequentially, first activating the API retriever and then the solver. Owing to\n",
      "the hierarchical design of our API retriever, we systematically re-activate various agents, following an ascending order from\n",
      "tool agents, to category agents, and finally to the meta-agent. The prompts for re-activating the tool agents, the category\n",
      "agents and the meta-agent are presented in Table 18, Table 19, and Table 20, respectively.\n",
      "B.4. Prompt for Creating AnyToolBench\n",
      "This can be found in Table 21.\n",
      "C. Case Study\n",
      "In Figure 7, we present a case study that demonstrates the process of resolving a user query using AnyTool. The self-\n",
      "reflection mechanism reactivates the tool, category, and the meta agents sequentially. It is worth noting that not all agents\n",
      "are reactivated. Subsequently, the solver is reactivated to attempt addressing the user query again, utilizing the updated API\n",
      "candidate pool. This self-reflection mechanism can be employed multiple times until the termination criteria are met—either\n",
      "the query is regarded as solved by the evaluator, or the number of self-reflections reaches the maximum limit.\n",
      "15\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Table 21: Prompt for Creating AnyToolBench.\n",
      "Your task is to interact with a sophisticated database of tools and functions,\n",
      "often referred to as APIs, to construct a user query that will be answered\n",
      "using the capabilities of these APIs. This database is organized into various\n",
      "categories, indicated by {categories }. To guide your exploration and selection\n",
      "of the appropriate APIs, the database offers several meta functions:\n",
      "Exploration Functions:\n",
      "1. Use get tools incategory to explore tools in a specific category.\n",
      "2. Employ get apis intool to discover the list of APIs available within a\n",
      "selected tool.\n",
      "3. If you need detailed information about a tool, get tool descriptions will\n",
      "provide it.\n",
      "4. For in-depth understanding of an API’s functionality, turn to\n",
      "getapidetails.\n",
      "Selection and Testing Functions:\n",
      "1. As you identify relevant functions, add them to your working list using\n",
      "addapis into apipool.\n",
      "2. Test these functions by synthesizing and applying various parameters.\n",
      "This step is crucial to understand how these functions can be practically\n",
      "applied in formulating your query.\n",
      "3. Should you find any function obsolete or not fitting your query context,\n",
      "remove them using remove apis from apipool.\n",
      "Query Formulation Guidelines:\n",
      "1.Your formulated query should be comprehensive, integrating APIs from 2\n",
      "to 5 different categories. This cross-functional approach is essential to\n",
      "demonstrate the versatility and broad applicability of the database.\n",
      "2.Avoid using ambiguous terms. Instead, provide detailed, specific\n",
      "information. For instance, if your query involves personal contact details,\n",
      "use provided placeholders like {email }for email, {phone number }for phone\n",
      "number, and URLs like {url}for a company website.\n",
      "3.The query should be relatable and understandable to users without requiring\n",
      "knowledge of the specific tools or API names used in the background. It\n",
      "should reflect a real-world user scenario.\n",
      "4. Aim for a query length of at least thirty words to ensure depth and\n",
      "complexity.\n",
      "Final Steps:\n",
      "1.Once you’ve crafted the query, use the Finish function to submit it along\n",
      "with the corresponding answer. The answer should be direct and concise,\n",
      "addressing the query without delving into the operational plan of the APIs.\n",
      "2.Remember, the total number of calls to the initial meta functions should not\n",
      "exceed 20.\n",
      "3.Consider various use cases while formulating your query, such as data\n",
      "analysis in business contexts or educational content in academic settings.\n",
      "Your approach should be creative and inclusive, catering to users with\n",
      "different skill levels and cultural backgrounds. Ensure that the query is\n",
      "globally relevant and straightforward, serving a singular purpose without\n",
      "diverging into unrelated areas. The complexity of your query should stem from\n",
      "the synthesis of information from multiple APIs.\n",
      "16\n",
      "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
      "Query\n",
      "“I'm organizing a charity event to raise awareness for animal rights. Can you recommend a book that highlights the \n",
      "importance of compassion towards animals? Additionally, provide me with a random word that symbolizes unity and \n",
      "empathy.”\n",
      "Meta -Agent\n",
      "1.Call get_tools_descriptions  (['GetBooksInfo ', 'Book \n",
      "Finder', 'Random Word', 'Dictionary', 'Master \n",
      "Dictionary', 'Random Words', 'Random Ukrainian \n",
      "Word', 'Random Words - Spanish and French'])\n",
      "2.Call create_agent_tool_level([' GetBooksInfo ', 'Book \n",
      "Finder']) →Create  Tool-Agent -1\n",
      "3.Call create_agent_tool_level(['Random Word', \n",
      "'Random Words', 'Random Ukrainian Word', \n",
      "'Random Words - Spanish and French’]) → Create  \n",
      "Tool-Agent -2\n",
      "4.Call create_agent_tool_level(['Dictionary', 'Master \n",
      "Dictionary']) → Create  Tool-Agent -3\n",
      "5.Call finish_search ()\n",
      "Category -Agent ( 'Education ')\n",
      " Category -Agent ( 'Text Analysis' )Bootstrap Prompt for the API Retriever \n",
      "1.Call get_tools_in_category ('Education') \n",
      "2.Call get_tools_in_category ('Other')                       \n",
      "3.Call create_agent_category_level (‘Education’) →Create  Category -Agent ( 'Education ')  \n",
      "4.Call get_tools_in_category ('Text Analysis')\n",
      "5.Call create_agent_category_level (‘Text Analysis’)  →Create  Category -Agent ( 'Text Analysis' )\n",
      "6.Call finish_search ()\n",
      "1.Call get_tools_descriptions  (['Random Word by API -\n",
      "Ninjas')\n",
      "2.Call get_tools_descriptions  (['Synonyms Words'])\n",
      "3.Call get_tools_descriptions  (['Best Paraphrasing \n",
      "API'])\n",
      "Tool-Agent -1\n",
      "1.Call add_API_into_API_pool (['Search','Find  \n",
      "books']) → Add APIs\n",
      "2.Call check_if_request_solvable ()\n",
      "Tool-Agent -2\n",
      "1.Call add_API_into_API_pool  (['wordSearchEnglish ', \n",
      "'Word infos '])→ Add APIs\n",
      "2.Call check_if_request_solvable ()\n",
      "Tool-Agent -3\n",
      "1.Call add_API_into_API_pool  (['run. cgi','Get a \n",
      "random word']) → Add APIs\n",
      "2.Call check_if_request_solvable ()\n",
      "API C andidate Pool\n",
      "'Search'              'Find books’               ' wordSearchEnglish '            'Word infos '          ' run.cgi '           'Get a random word'                     \n",
      "Bootstrap Prompt for the S olver\n",
      "Solver\n",
      "“Here is a book that might interest you: 'Animal Spy: Animal Welfare Behind Enemy Lines' by Terry Spamer  & Gordon \n",
      "Thorburn. However, due to the query limit, I couldn't generate a random word that symbolizes unity and empathy. \n",
      "Apologies for the inconvenience.”\n",
      "Bootstrap Prompt for the S elf-Reflection Mechanism\n",
      "Tool-Agent -1,Tool-Agent -2, Tool -Agent -3,\n",
      "Category -Agent ( 'Text_analysis ')\n",
      "Solver\n",
      "\"Here are some book recommendations that highlight the importance of compassion towards animals:  'Animal Spy: \n",
      "Animal Welfare Behind Enemy Lines' by Terry Spamer , Gordon Thorburn. A book by Leslie Irvine on the effect of disasters \n",
      "on animals. The random word generated is 'transcendental', signifying elevated thinking and broader understanding, which \n",
      "might be seen as relevant to unity and empathy in a broader sense.\"Updated API C andidate Pool\n",
      "'Find books', 'Get multiple random words’,  'Retrieve Response'Re-activate: \n",
      "Figure 7: Illustration of a case study.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "page_summaries = []\n",
    "for i in range(len(pdf_docs)):\n",
    "    print(pdf_docs[i].page_content)\n",
    "    page_summary_prompt = f\"Summarize the following page of a paper in one simple paragraph: {pdf_docs[i].page_content}.\"\n",
    "    page_summaries.append(get_response(page_summary_prompt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnyTool is a new language model agent that utilizes over 16,000 APIs to address user queries, using a hierarchical structure, a solver, and a self-reflection mechanism. AnyTool is powered by the function calling feature of GPT-4 and outperforms existing models such as ToolLLM and a GPT-4 variant in tool utilization. The authors also introduce a new evaluation protocol, AnyToolBench, to better reflect practical application scenarios. Experiments show that AnyTool surpasses existing models in addressing user queries by effectively utilizing a large pool of APIs.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n",
      "864\n",
      "848\n",
      "1017\n",
      "727\n",
      "755\n",
      "803\n",
      "884\n",
      "818\n",
      "666\n",
      "899\n",
      "1054\n",
      "867\n",
      "790\n",
      "858\n",
      "779\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "for summary in page_summaries:\n",
    "    print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_summaries = \" \".join(s for s in page_summaries[:len(page_summaries)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AnyTool is a new language model agent that utilizes over 16,000 APIs to address user queries, using a hierarchical structure, a solver, and a self-reflection mechanism. AnyTool is powered by the function calling feature of GPT-4 and outperforms existing models such as ToolLLM and a GPT-4 variant in tool utilization. The authors also introduce a new evaluation protocol, AnyToolBench, to better reflect practical application scenarios. Experiments show that AnyTool surpasses existing models in addressing user queries by effectively utilizing a large pool of APIs. AnyTool is a hierarchical API retriever that aims to identify the most relevant APIs to user queries from a large pool, using a self-reflection mechanism to improve efficiency and effectiveness. It has a hierarchical structure composed of a meta-agent, category agents, and tool agents, each with specific functions to explore the API space. AnyTool\\'s design is based on the dual-phase approach of previous research, but it introduces four distinct characteristics: it doesn\\'t require training. It has a hierarchical structure, a self-reflection mechanism, and it\\'s plug-and-play. The self-reflection mechanism involves an initial attempt at solving a query, followed by reflection and re-activation if the solution is not feasible. This mechanism reduces over-searching for simpler queries and provides a more context-rich and in-depth search for complex queries. The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" presents AnyTool, a tool that utilizes GPT-4 as an API retriever with a hierarchical structure and a self-reflection mechanism to improve the accuracy of API retrieval for large-scale API calls. AnyTool outperforms existing strong baselines in tool utilization in large language models (LLMs), such as ToolLLM and a version of GPT-4 tailored for tool utilization. The paper also discusses the limitations of existing evaluation methodologies in tool utilization and proposes a revised evaluation protocol. The authors introduce an additional benchmark, AnyToolBench, to facilitate the application of the new evaluation protocol. AnyTool addresses the limitations of previous approaches by providing a feedback mechanism and improving the accuracy of API retrieval. The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" presents an evaluation protocol improvement for using APIs in solving user queries. The initial method (ToolLLM) involves an API retriever that selects relevant API candidates, and GPT-4 evaluates their solvability and effectiveness. However, this protocol can result in high pass rates even for unsolved queries if the API retriever selects unrelated candidates. The proposed solution is to directly use GPT-4 to determine the efficacy of the agent\\'s proposed solution, bypassing the initial evaluation phase. Additionally, the authors introduce AnyTool, a new approach that does not require external module training, can search through a large API pool (16K+) using a hierarchical structure, and is capable of self-reflection for unsolved queries. AnyTool consists of three steps: creating an API candidate pool, attempting to resolve the query using these candidates, and engaging in self-reflection if the query remains unsolved. AnyTool is a system for large-scale API calls, divided into three tiers: category level, tool level, and individual APIs. The system consists of a meta-agent, category agents, and tool agents, each with distinct functions. The system generates an API-candidate pool by calling functions using GPT-4\\'s function-calling feature. The solver, a singular agent, addresses the user\\'s query using the generated API candidate pool. The self-reflection mechanism re-activates AnyTool if the initial solution fails to resolve user queries. The self-reflection mechanism can be applied repeatedly until the termination condition is met. The API retriever\\'s hierarchical design allows for systematic re-activation of agents for efficiency. The paper compares the performance of various models, including ToolLLM, its variants, and several GPT-4 models, in addressing user queries by utilizing APIs. The models are evaluated based on their pass rate in solving queries from two benchmarks, ToolBench and AnyToolBench. The results show that AnyTool, an agent with a self-reflective and hierarchical design, outperforms other models in solving queries by effectively expanding the API-candidate pool and re-activating appropriate agents. The evaluation is conducted using GPT-4 as a judge to assess the feasibility of generated solutions. AnyTool demonstrates a higher pass rate in solving queries compared to ToolLLM and its variants, as well as various GPT-4 models tailored for tool utilization. The paper introduces AnyTool, a self-reflective, hierarchical agent system that outperforms existing models in handling large-scale API calls. AnyTool integrates a hierarchical structure and a self-reflection mechanism, leading to a 32.6 point increase and a 19.3 point increase, respectively, in the pass rate compared to ToolLLM and GPT-4. The hierarchical structure involves dividing the set of over 16,000 APIs into groups for processing by GPT-4, while the self-reflection mechanism allows the system to operate in a closed loop. The ablation studies highlight the importance of the hierarchical structure and self-reflection mechanism in AnyTool\\'s performance. AnyTool surpasses other models in resolving user queries using the entire API pool, as it does not supply reference APIs for each query. The paper discusses the system \"AnyTool,\" which utilizes a large-scale API pool to address user queries by selecting relevant APIs. The API pool\\'s size was studied, with results indicating that larger pools (e.g., 10,000 APIs) lead to higher pass rates. The maximal size of the API-candidate pool was also examined, with a pool size of 64 offering near-saturation performance. The paper introduces a two-step process in which the solver uses an API-candidate pool generated by a hierarchical API Retriever. The impact of the maximal number of tools a tool agent can manage was studied, showing a trade-off between managing many tools (higher recall) and fewer tools (smaller search space). AnyTool\\'s self-reflection frequency and agent quantity were analyzed, concluding that AnyTool enhances its proficiency in responding to user queries through its unique self-reflection mechanism.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half_summary = get_response(f\"Summarize the following summaries of a paper into one comprehensive yet simple summary with all the information from the paper:\\n\\n{first_summaries}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_summaries = \" \".join(s for s in page_summaries[len(page_summaries)//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" discusses an open-source Language Learning Model (LLM) designed for API utilization, aiming to facilitate local deployments. However, its performance in highly complex scenarios is yet to be verified due to the lack of appropriate datasets. The model\\'s effectiveness depends on the capabilities of GPT-4, as it relies on its function-calling feature. The paper references various studies and models, such as GPT-4, GPT-3, PaLM, and others, which have contributed to the development of language models and their applications. Additionally, it cites works on tool learning, large language models as tool makers, and tool usage with large language models, which highlight the potential of integrating tools and APIs with language models. The paper summarizes recent research on developing and utilizing large language models for various applications, including API calls and real-world applications. The authors discuss tools such as AnyTool, HuggingGPT, RestGPT, ToolAlpaca, LaMDA, Llama, Llama 2, Self-instruct, and Self-rewarding language models, which demonstrate the potential of large language models in solving AI tasks, aligning models with self-generated instructions, and enabling tool manipulation capabilities. The papers also explore the use of chain-of-thought prompting to elicit reasoning in large language models and the development of more open and efficient foundation language models. The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" presents a system that uses hierarchical agents to perform large-scale API calls. The agents include Meta Agent, Category Agent, and Tool Agent, each with specific functions such as creating agents, getting tools and descriptions, adding APIs, and checking request solvability. The system uses DFSDT (Dynamic Filtering, Selection, and Distribution of Traversal) solver and a self-reflection mechanism. The paper compares the performance of AnyTool with ToolLLM models on the original ToolBench, which includes non-solvable queries. AnyTool outperforms ToolLLM models in solving the queries. The implementation details include setting a maximum number of API calls and a token limit for efficiency. The paper also compares the performance of AnyTool with a GPT-4 variant that uses a plain agent as the API retriever. The paper compares the performance of AnyTool and GPT-4 in retrieving APIs using different plain agents and AutoGen-RAG agents. The plain agents vary in the level of detail they can access, while the AutoGen-RAG agents use different embedding models. AnyTool outperforms the other agents, achieving 58.9% and 63.2% in G2-I and G3-I, respectively. The paper also discusses the Retrieval-augmented generation (RAG) method, which combines an input prompt with relevant documents from a reference to enhance the performance of large language models (LLMs) in providing accurate factual information. AnyTool\\'s consumption analysis shows an average of 13.5×104 tokens, 14.1 API candidates, 43.3 OpenAI API calls, and 4.6 self-reflections per query. However, calculating the processing time for resolving queries with AnyTool is not feasible due to the unstable server response of GPT-4\\'s function-calling feature. The paper also mentions the filtering process for ToolBench, focusing on eliminating non-solvable queries based on essential information criteria. The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" discusses the creation of AnyToolBench, a benchmark for evaluating AI models in their ability to utilize APIs for problem-solving. The authors use GPT-4 to construct the benchmark by exploring an API pool, selecting relevant APIs, generating required parameters, and formulating queries. The API pool is then verified to ensure that every query is solvable. The paper also introduces the concept of a meta-agent, composed of category agents and tool agents, each with their own bootstrap prompts. The paper includes Tables 12 and 13, displaying consumption statistics for each dataset and examples of the AnyToolBench, respectively. The paper aims to address the challenges of using APIs for problem-solving, such as unreasonable queries, non-existent URLs, and overly broad queries. The paper discusses a system called AnyTool, which consists of hierarchical agents to help users find relevant categories and tools for large-scale API calls. There are three types of agents: meta-agent, category agent, and tool agent. The meta-agent identifies relevant categories using the \\'query tools incategory\\' function and assigns categories to agents. Category agents help find relevant tools within a specific category, while tool agents find relevant APIs within tools. The tool agent adds relevant APIs to the API pool using the \\'add apis into apipool\\' function. The Solver has a prompt that includes a \"give up\" option with a reason when choosing \"give up and restart.\" The purpose is to assist users in finding relevant categories, tools, and APIs within the AnyTool hierarchy. The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" presents a method for using a system called AnyTool to solve tasks by making API calls. AnyTool has a self-reflective mechanism that allows it to analyze its progress and make decisions on what to do next. It does this by making function calls to execute each step of the task. If the current APIs fail to solve the query, AnyTool will seek additional APIs or assign more unexplored tools and categories to the agents. The self-reflection mechanism re-activates AnyTool sequentially, starting with the API retriever and then the solver, and follows an ascending order from tool agents, to category agents, and finally to the meta-agent. The process continues until the query is regarded as solved by the evaluator or the number of self-reflections reaches the maximum limit. The paper discusses a tool called AnyTool that allows users to interact with a large database of APIs to construct a user query. The database is organized into categories and offers exploration functions to help users find tools and APIs, as well as selection and testing functions to manage the APIs. When creating a query, users should integrate APIs from 2 to 5 different categories, avoid ambiguous terms, make the query relatable and understandable, and aim for a query length of at least thirty words. The total number of calls to the initial meta functions should not exceed 20. The final answer should be direct and concise, without delving into the operational plan of the APIs. The query should consider various use cases, be creative, inclusive, and globally relevant. The paper presents a tool called AnyTool that uses hierarchical, self-reflective agents to make large-scale API calls. In this example, the user is looking for a book recommendation and a random word related to unity and empathy for a charity event. The Meta-Agent receives the query and creates three Tool-Agents for finding books and random words in different languages, and for using a dictionary. These agents then create Category-Agents for \\'Education\\' and \\'Text Analysis\\' categories. The agents then search for appropriate APIs and check if the request is solvable. The Solver then provides the book recommendation and a random word, and the Self-Reflection Mechanism evaluates the results. The API Candidate Pool is then updated, and the process can be re-activated as needed.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_half_summary = get_response(f\"Summarize the following summaries of a paper into one comprehensive yet simple summary with all the information from the paper:\\n\\n{second_summaries}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AnyTool is a new language model agent that utilizes GPT-4's function-calling feature to address user queries by calling APIs from a large pool of over 16,000. It has a hierarchical structure with a self-reflection mechanism, making it more efficient and effective in API retrieval than existing models like ToolLLM and a GPT-4 variant. AnyTool consists of a meta-agent, category agents, and tool agents that search the API space in a hierarchical manner. It improves the accuracy of API retrieval and addresses the limitations of previous evaluation methodologies. AnyTool outperforms existing models in solving user queries by effectively utilizing a large pool of APIs and has a higher pass rate in addressing queries. It also introduces a new evaluation protocol, AnyToolBench, to better reflect practical application scenarios. AnyTool's self-reflection mechanism reduces over-searching for simpler queries and provides a more context-rich and in-depth search for complex queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(first_half_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\" introduces AnyTool, a system using hierarchical, self-reflective agents for large-scale API calls to solve complex problems. AnyTool consists of Meta, Category, and Tool agents, and a Solver with a self-reflection mechanism. It is designed for API utilization, relying on the function-calling feature of GPT-4. AnyTool outperforms other models in API retrieval and problem-solving, creating a benchmark for AI models, AnyToolBench, using GPT-4. The system is effective for complex scenarios, with limitations in efficiency and processing time due to GPT-4's unstable server response. AnyTool is used to make large-scale API calls, interact with a database of APIs, and solve tasks by making function calls to execute steps of the task. It is useful for tasks requiring API calls, with a focus on user queries, exploration, selection, and testing of APIs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(second_half_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automations",
   "language": "python",
   "name": "automations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
